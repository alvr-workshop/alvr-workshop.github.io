<!-- FlatFy Theme - Andrea Galanti /-->
<!doctype html>
<!--[if lt IE 7]> <html class="no-js ie6 oldie" lang="en"> <![endif]-->
<!--[if IE 7]>    <html class="no-js ie7 oldie" lang="en"> <![endif]-->
<!--[if IE 8]>    <html class="no-js ie8 oldie" lang="en"> <![endif]-->
<!--[if IE 9]>    <html class="no-js ie9" lang="en"> <![endif]-->
<!--[if gt IE 9]><!--> <html> <!--<![endif]-->
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, user-scalable=no, initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0">
    <meta name="description" content="Workshop on Advances in Language and Vision Research ">
    <meta name="author" content="">

    <title>ALVR 2024</title>

    <!-- Bootstrap core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom Google Web Font -->
    <link href="font-awesome/css/font-awesome.min.css" rel="stylesheet">
    <link href='http://fonts.googleapis.com/css?family=Lato:100,300,400,700,900,100italic,300italic,400italic,700italic,900italic' rel='stylesheet' type='text/css'>
    <link href='http://fonts.googleapis.com/css?family=Arvo:400,700' rel='stylesheet' type='text/css'>

    <!-- Custom CSS-->
    <link href="css/general.css" rel="stylesheet">

    <!-- Owl-Carousel -->
    <link href="css/custom.css" rel="stylesheet">
    <link href="css/owl.carousel.css" rel="stylesheet">
    <link href="css/owl.theme.css" rel="stylesheet">
    <link href="css/style.css" rel="stylesheet">
    <link href="css/animate.css" rel="stylesheet">

    <!-- Magnific Popup core CSS file -->
    <link rel="stylesheet" href="css/magnific-popup.css">

    <script src="js/modernizr-2.8.3.min.js"></script> <!-- Modernizr /-->
    <!--[if IE 9]>
    <script src="js/PIE_IE9.js"></script>
    <![endif]-->
    <!--[if lt IE 9]>
    <script src="js/PIE_IE678.js"></script>
    <![endif]-->

    <!--[if lt IE 9]>
    <script src="js/html5shiv.js"></script>
    <![endif]-->

</head>

<body id="home">

<!-- Preloader -->
<!--
<div id="preloader">
    <div id="status"></div>
</div>
-->

<!-- FullScreen -->
<div class="intro-header">
    <div class="col-xs-12 text-center">
        <div style="background-color: rgba(0,0,0,0.5)">
        <h1 class="h1_home wow " data-wow-delay="0.4s" style="color:white; text-shadow: 0 0 8px black;">3<sup>rd</sup> Workshop on Advances in Language and Vision Research (ALVR)</h1>
        <h3 class="h3_home wow " data-wow-delay="0.6s" style="color:white; text-shadow: -1px 0 black, 0 1px black, 1px 0 black, 0 -1px black, 2px 2px 4px black">
            In conjunction with <a style="color:white" href="https://2024.aclweb.org/" target="_blank"><b>ACL 2024</b></a> <br>
            August 15~16<sup>st</sup> 2024 (Full Day) <br>
            Location: Bangkok, Thailand
        </h3>
        </div>
        <!--
        <h3 class="h3_home wow " data-wow-delay="0.6s" style="color:black; background-color:rgba(255,255,255,0.5); text-shadow: -1px 0 black, 0 1px black, 1px 0 black, 0 -1px black, 2px 2px 4px black">
            In conjunction with <a style="color:black" href="https://2024.naacl.org/" target="_blank"><b>NAACL 2024</b></a> <br>
            June 11<sup>st</sup> 2024 (Full Day) <br>
            Location: Mexico City, Mexico
        </h3>
        -->
        <!--ul class="list-inline intro-social-buttons">
            <li><a href="https://twitter.com/galantiandrea" class="btn  btn-lg mybutton_cyano wow " data-wow-delay="0.8s"><span class="network-name">Twitter</span></a>
            </li>
            <li id="download" ><a href="#downloadlink" class="btn  btn-lg mybutton_standard wow swing wow " data-wow-delay="1.2s"><span class="network-name">Free Download</span></a>
            </li>
        </ul-->
    </div>
    <!-- /.container -->
    <div class="col-xs-12 text-center abcen wow ">
        <div class="button_down ">
            <a class="imgcircle wow bounceInUp" data-wow-duration="1.5s"  href="#scope"> <img class="img_scroll" src="img/icon/circle.png" alt=""> </a>
        </div>
    </div>
    <span class="intro-caption">Photo by <a href="https://unsplash.com/@manuel_arroyo25">Manuel Arroyo</a> on <a href="http://www.unsplash.com">Unsplash</a> </span>
</div>

<!-- NavBar-->
<nav class="navbar-default" role="navigation">
    <div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="#home">ALVR 2024</a>
        </div>

        <div class="collapse navbar-collapse navbar-right navbar-ex1-collapse">
            <ul class="nav navbar-nav">
                <li class="menuItem"><a href="#scope">Home</a></li>
                <!-- <li class="menuItem"><a href="#dates">Dates</a></li> -->
                <!-- <li class="menuItem"><a href="#submit">Submission</a></li> -->
                <!-- <li class="menuItem"><a href="#proceedings">Proceedings</a></li> -->
                <li class="menuItem"><a href="#program">Schedule</a></li>
                <!-- <li class="menuItem"><a href="#accepted-papers">Accepted Papers</a></li> -->
                <li class="menuItem"><a href="#invited">Invited Speakers</a></li>
                <li class="menuItem"><a href="#organizers">Organizers</a></li>
                <!-- <li class="menuItem"><a href="#sponsors">Sponsor</a></li>					 -->
                <!-- <li class="menuItem"><a href="#contacts">Contact</a></li> -->
                <li class="menuItem"><a href="2020/index.html">ALVR 2020</a></li>
                <li class="menuItem"><a href="2021/index.html">ALVR 2021</a></li>
            </ul>
        </div>

    </div>
</nav>

<!-- Scope -->
<div id ="scope" class="content-section-a" style="border-top: 0">

    <div class="container">

        <div class="row">

            <!--div class="col-sm-6 pull-right wow ">
                <img class="img-responsive " src="img/ipad.png" alt="">
            </div-->
            <!--<img  class="img-responsive"  style="width: 20vw"  src="img/cvpr_logo.png" alt="">
            <img  class="img-responsive"  style="width: 20vw"  src="img/cvf.jpg" alt="">
            <img  class="img-responsive"  style="width: 10vw"  src="img/organizer/vgis.png" alt="">  -->
            <div class="wow " data-animation-delay="200">
                <!--<h3 class="section-heading">ACL 2020 Workshop on Advances in Language and Vision Research</h3>-->
                <h3 class="section-heading">3<sup>rd</sup> Workshop on Advances in Language and Vision Research</h3>

                <!--div class="sub-title lead3">Lorem ipsum dolor sit atmet sit dolor greand fdanrh<br> sdfs sit atmet sit dolor greand fdanrh sdfs</div-->
                <p>
                    Language and vision research has attracted great attention from both natural language processing (NLP) and computer vision (CV) researchers. Gradually, this area is shifting from passive perception, templated language, and synthetic imagery/environments to active perception, natural language, and photo-realistic simulation or real world deployment. 
                    <!--
                    Thus far, few workshops on language and vision research have been organized by groups from the NLP community. 
                    We are organizing <b>the second workshop on Advances in Language and Vision Research (ALVR)</b> in order to promote the frontier of language and vision research and to bring interested researchers together to discuss how to best tackle and solve real-world problems in this area.<br/><br/>
                    -->
                    This workshop covers (but is not limited to) the following topics: <br/>
                <ul>
                    <li> Self-supervised vision and language pre-training; </li>
                    <li> New tasks and datasets that provide real-world solutions in language and vision; </li>
                    <li> Text-to-image/video generation and text-guided image/video editing; </li>
                    <li> External knowledge integration in visual and language understanding;</li>
                    <li> Visually-grounded natural language understanding and generation;</li>
                    <li> Language-grounded visual recognition and reasoning;</li>
                    <li> Language-grounded embodied agents, e.g., vision-and-language navigation;</li> 
                    <li> Visually-grounded multilingual study, e.g., multimodal machine translation; </li>
                    <li> Shortcomings of the existing large vision\&language models on downstream tasks and solutions;</li> 
                    <li> Ethics and bias on large vision\&language model.</li>
                    <li> Multidisciplinary study that may involve linguistics, cognitive science, robotics, etc.</li>
                    <li> Explainability and interpretability on large vision\&language model.</li>
                </ul>
                </p>

                <!-- <a id="important-dates" class="anchor" href="#accepted-papers" aria-hidden="true"><span class="octicon octicon-link"></span></a>_..

                 <!--p><a class="btn btn-embossed btn-primary" href="#" role="button">View Details</a>
                 <a class="btn btn-embossed btn-info" href="#" role="button">Visit Website</a></p-->
            </div>
        </div>
    </div>
    <!-- /.container -->
</div>

<!--
<div id ="dates" class="content-section-b" style="border-top: 0">
    <div class="container">
        <div class="row">
            <div class="wow " data-animation-delay="200">
                <h3 class="section-heading">Important Dates</h3>
                <ul>
                    <li>Archival track:</li>
                    <ul>
                      <li><s>Paper Submission Due Date: March 22, 2024</s></li>
                      <li><s>Notification of acceptance: April 15, 2024</s></li>
                        <li><s>Camera-ready papers due: April 26, 2024</s></li>
                    </ul>
                    <li>Non-archival track:</li>
                    <ul>
                        <li><s>Paper Submission Due Date: April 30, 2024</s></li>
                        <li><s>Notification of acceptance: May 14, 2024</s></li>
                        <li><s>Camera-ready papers due: May 24, 2024</s></li>
                    </ul>
                    <li>Workshop Date: June 11, 2024</li>
                </ul>
            </div>
        </div>
    </div>
</div> 
-->
<!-- /dates -->

<!-- Call for Papers -->
<!--
<div id ="submit" class="content-section-b">

    <div class="container">

        <div class="row">

            <div class="wow " data-animation-delay="200">
                <h3 class="section-heading">Submission</h3>

                <p>The workshop includes an archival and a non-archival track on topics related to language-and-vision research. For both tracks, the reviewing process is single-blind. That is, the reviewer will know the authors but not the other way around. Submission is electronic, using the Softconf START conference management system. The submission site will be available at <a href="https://www.softconf.com/naacl2024/alvr2024" style="color:orange">https://www.softconf.com/naacl2024/alvr2024</a>.</p>

                <h4 class="section-heading">Archival Track</h4>
                <p>
                    The archival track follows the NAACL short paper format
                    (<a href="https://2024.naacl.org/calls/papers/#short-papers" style="color:orange">https://2024.naacl.org/calls/papers/#short-papers</a>).
                    Submissions to the archival track may consist of up to 4 pages of content (excluding references) in
                    NAACL format (style sheets are available below), plus extra space for an optional ethics/broader
                    impact statement and unlimited references. Accepted papers will be given 5 content pages for the
                    camera-ready version. Authors are encouraged to use this additional page to address reviewers’
                    comments in their final versions. The accepted papers to the archival track will be included in the
                    ACL 2024 Workshop Proceedings. The archival track does not accept double submissions, e.g., no
                    previously published papers or concurrent submissions to other conferences or workshops.
                </p>
                <p>
                    The format of submitted papers to the archival track must follow the NAACL Author Guidelines.
                    Style sheets (Latex, Word) are available here:
                    <a href="https://2024.naacl.org/calls/style-and-formatting/" style="color:orange">https://2024.naacl.org/calls/style-and-formatting/</a>
                </p>

                <h4 class="section-heading">Non-archival Track</h4>
                <p>
                    The workshop also includes a non-archival track to allow submission of previously published papers and
                    double submission to ALVR and other conferences or journals. Accepted non-archival papers can still be
                    presented as posters at the workshop.
                </p>

                <p>
                    There are no formatting or page restrictions for non-archival submissions. The accepted papers to the
                    non-archival track will be displayed on the workshop website, but will NOT be included in the NAACL 2024
                    Workshop proceedings or otherwise archived.
                </p>

            </div>

        </div>
    </div>
    <!-- /.container -->
</div>
-->
<!--
<div id ="proceedings" class="content-section-b">

    <div class="container">

        <div class="row">

            <div class="wow " data-animation-delay="200">
                <h3 class="section-heading">Proceedings</h3>
                <li>TBD</li>
            </div>
        </div>
    </div>
</div>
-->

<!-- Program -->
<div id ="program" class="content-section-b">

    <div class="container">
        <div class="row">
            <div class="wow " data-animation-delay="200">
                <h3 class="section-heading">Schedule (ICT, UTC+7)</h3>
                <p>To be updated</p>
                <!--<p class="lead">
                    <b>ATTENTION! The video recording of the whole workshop is located at <a href="https://slideslive.com/38931798/w9-alvr-live-stream" style="color:orange"> <u>https://slideslive.com/38931798/w9-alvr-live-stream</u></a></b>. Some pre-recorded talks are listed below, while others are included in the workshop recording.
                </p>-->
            </div>

            <!-- <table class="table" style="width:100%; table-layout:fixed">
                <tr>
                    <th scope="col" width="15%">Time (PDT) </th>
                    <th scope="col" width="60%">Event </th>
                    <th scope="col" width="20%">Who </th>
                    </th>

                <tr class="info">
                    <td>8:30-8:35</td>
                    <td><strong>Opening Remarks</strong></td>
                    <td>Workshop Organizers</td>
                </tr>

                <tr class="success">
                    <td>8:35-9:10</td>
                    <td>Invited Talk 1: <strong>Instructions, Abstraction, and Theory-of-Mind</strong>
                      <a class="talk-toggle" data-toggle="collapse" data-target="#schedule-talk1">Abstract</a>
                      <div class="collapse" id="schedule-talk1">This talk focuses on an overview of our recent environments and benchmarks: ALFRED and ALFWorld for instruction following in embodied and abstract action spaces. The goal is to help move the community towards building agents that connect language to action and understand abstract plans. As we move towards systems which interact with the world, we also need to think about how they interact with other agents. I close with a quick preview of upcoming work at ICML on Theory-of-Mind agents based in the ALFWorld environment.</div>
                    </td>
                    <td>Yonatan Bisk</td>
                </tr>

                <tr class="success">
                    <td>9:10-9:45</td>
                    <td>Invited Talk 2: <strong>Generalization in Vision and Language Reasoning</strong>
                      <a class="talk-toggle" data-toggle="collapse" data-target="#schedule-talk2">Abstract</a>
                      <div class="collapse" id="schedule-talk2">A key challenge for Artificial Intelligence research is to go beyond static observational data, and consider more challenging settings that involve dynamic actions and incremental decision-making. In this talk, I will introduce our recent work on visually-grounded language reasoning via the studies of vision-and-language navigation. In particular, I will emphasize two main benefits of self-supervised learning that improve generalization by (1) creating counterfactuals to augment observational data; (2) enables transfer learning for challenging settings. I will present our empirical results on indoor and outdoor navigation datasets, and demonstrate the effectiveness of our proposed Adversarial Path Sampler and Multimodal Text Style Transfer approaches for vision-and-language navigation.</div>
                    </td>
                    <td>William Wang</td>
                </tr>
                
                <tr class="info">
                    <td>9:45-10:10</td>
                    <td><strong>Poster Highlight</strong><br /></td>
                    <td></td>
                </tr>
            
                <tr>
                    <td>10:10-10:50</td>
                    <td><strong>Poster Session 1</strong><br /></td>
                    <td></td>
                </tr>

                <tr class="success">
                    <td>10:50-11:25</td>
                    <td>Invited Talk 3: <strong>If Bears were Bees and Cats were Researchers</strong>
                      <a class="talk-toggle" data-toggle="collapse" data-target="#schedule-talk3">Abstract</a>
                      <div class="collapse" id="schedule-talk3">Human knowledge and use of language is inextricably connected to perception, action and the organization of the brain, yet natural language processing is still dominated by text! More research involving language---including speech---in the context of other modalities and environments is needed, and there has never been a better time to do it. In my talk, I’ll share some of my team’s work on these topics, particularly around language-image retrieval, text-to-image generation and vision and language navigation.
                        <br><br>
                        That said, this talk will mostly be a Complaining Discussion about vision and language research. Rather than giving a glossy and triumphant overview of some papers, I’d like to include additional observations and questions regarding current research business-as-usual, injecting my own split perspective as an empiricist in natural language processing and as a person with both anthropological and linguistic training, perspectives and predilections. My hope is that through some Complaining and Introspection, I can provide useful reminders of why this research area is still so hard, so wide open, so important and so exciting.
                      </div>
                      </td>
                    <td>Jason Baldridge</td>
                </tr>

                <tr class="success">
                    <td>11:25-12:00</td>
                    <td>Invited Talk 4: <strong>Action Learning and Justification Using Language</strong>
                      <a class="talk-toggle" data-toggle="collapse" data-target="#schedule-talk4">Abstract</a>
                      <div class="collapse" id="schedule-talk4">Task learning from natural language instructions involves understanding, learning, and justification of perceived actions. In this talk, I will talk about some work done in my lab that incorporates commonsense knowledge in language grounding, action learning and justification. I will also introduce our recent work on hierarchical task learning using the ALFRED dataset and discuss key challenges and opportunities.</div>
                    </td>
                    <td>Joyce Chai</td>
                </tr>
            
                <tr class="info">
                    <td>12:00-12:30</td>
                    <td><strong>Panel Session 1</strong><br /></td>
                    <td>Jason Baldridge, Joyce Chai, Kate Saenko, William Wang</td>
                </tr>

                <tr>
                    <td>12:30-13:20</td>
                    <td><strong>Lunch</strong><br /></td>
                    <td></td>
                </tr>

                <tr class="success">
                    <td>13:20-13:55</td>
                    <td>Invited Talk 5: <strong> Separating Skills and Concepts for Novel Visual Question Answering</strong>
                    </td>
                    <td>Kate Saenko</td>
                </tr>

                <tr class="success">
                    <td>13:55-14:30</td>
                    <td>Invited Talk 6: <strong>Natural Language Explanations of Deep Networks</strong>
                      <a class="talk-toggle" data-toggle="collapse" data-target="#schedule-talk6">Abstract</a>
                      <div class="collapse" id="schedule-talk6">Despite major efforts in recent years to improve explainability of deep neural networks, the tools we use for communicating explanations have largely remained the same: visualizations of representative inputs, salient input regions, and local model approximations. But when humans describe complex decision rules, we often use a different explanatory tool: natural language. I'll describe recent work on explaining models for computer vision tasks by automatically constructing natural language descriptions of individual neurons. These descriptions ground prediction in meaningful perceptual and linguistic abstractions, and can be used to surface unexpected model behaviors, identify adversarial vulnerabilities, and even guide text-based image editing. These results show that fine-grained, automatic annotation of deep network models is both possible and practical: rich, language-based explanations produced by automated annotation procedures can surface meaningful and actionable information about model behavior.</div>
                    </td>
                    <td>Jacob Andreas</td>
                </tr>

                <tr class="success">
                    <td>14:30-15:05</td>
                    <td>Invited Talk 7: <strong>Sherlock and Merlot: Multimodal Abductive Reasoning and Neural Script Knowledge</strong>
                      <a class="talk-toggle" data-toggle="collapse" data-target="#schedule-talk7">Abstract</a>
                      <div class="collapse" id="schedule-talk7">In this talk, I will present Sherlock, a new dataset and tasks for multimodal abductive reasoning, and Merlot, a new multimodal model for neural script knowledge that achieves SOTA on 12+ video-based QA benchmarks.</div>
                    </td>
                    <td>Yejin Choi</td>
                </tr>

                <tr>
                    <td>15:05-15:45</td>
                    <td><strong>Poster Session 2</strong><br /></td>
                    <td></td>
                </tr>

                <tr class="success">
                    <td>15:45-16:20</td>
                    <td>Invited Talk 8: <strong>Explanations for Visual Question Answering</strong>
                      <a class="talk-toggle" data-toggle="collapse" data-target="#schedule-talk8">Abstract</a>
                      <div class="collapse" id="schedule-talk8">This talk will review several aspects of our recent work on explanation for VQA. First, we have developed a VQA system that can elucidate its answers with multi-modal natural-language and visual explanations that faithfully reflect important aspects of its underlying reasoning while capturing the style of comprehensible human explanations. Crowd-sourced human evaluation of these explanations have demonstrated the advantages of our approach. Second, we have developed methods that use human-provided visual or textual explanations to aid the training of VQA systems and improve their robustness to changing problem distributions.  Finally, we have developed a novel framework that constructs explanations for multiple potential answers for a VQA problem and analyzes and compares these competing explanations to improve both the accuracy of the system as well as the quality of its explanations as evaluated by human judges.</div>
                    </td>
                    <td>Ray Mooney</td>
                </tr>

                <tr class="success">
                    <td>16:20-16:55</td>
                    <td>Invited Talk 9: <strong>New Frontiers in Vision and Language Research</strong>
                      <a class="talk-toggle" data-toggle="collapse" data-target="#schedule-talk9">Abstract</a>
                      <div class="collapse" id="schedule-talk9">Vision and Language research is rapidly evolving, in terms of both the methods and the applications. I will start by presenting a benchmarking study on the benefits of CLIP, a recent powerful multimodal model, for the traditional Vision and Language tasks. Then, I will talk about several new application scenarios for Vision and Language research. First, is our work on incorporating language into a formerly “vision-only” task of video summarization. Second, I will discuss how we can leverage language to address bias in visual classifiers. Lastly, I will talk about automatically generating and detecting out-of-context multimodal media, an emerging misinformation threat scenario.</div>
                    </td>
                    <td>Anja Rohrbach</td>
                </tr>

                <tr class="success">
                    <td>16:55-17:30</td>
                    <td>Invited Talk 10: <strong>Knowledgeable and Spatio-Temporal Vision+Language</strong>
                    </td>
                    <td>Mohit Bansal</td>
                </tr>

                <tr class="info">
                    <td>17:30-18:00</td>
                    <td><strong>Panel Session 2</strong><br /></td>
                    <td>Jacob Andreas, Mohit Bansal, Yejin Choi, Ray Mooney, Anja Rohrbach</td>
                </tr>

            </table> -->

            <div class="container">
                <div class="wow " data-animation-delay="200">
                </div>
            </div>
        </div>
    </div>
</div>

    

<!-- Invited Speakers -->
<div id ="invited" class="content-section-b">

    <div class="container">

        <div class="row">
            <div class="container">

                <div class="row wow "  data-animation-delay="200">
                    <h3 class="section-heading">Invited Speakers</h3>
                    <!--<p>(presentation order)</p>-->
                    <br>
                </div>

                <div class="row wow "  data-animation-delay="200">
                    <div class="col-sm-3 wow"  data-animation-delay="200">
                        <center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/invited/dhruv.jpg" alt=""></center>
                        <h4 class="section-heading"><center><a href="https://faculty.cc.gatech.edu/~dbatra/" target="blank">Dhruv Batra</a></center></h4>
                    </div>
                    <div class="col-sm-8 col-md-offset-1 wow" style="text-align:justify" data-animation-delay="200">
                        <p>Dhruv Batra is an Associate Professor in the School of Interactive Computing at Georgia Tech and a Research Director in the Fundamental AI Research (FAIR) team at Meta. His research interests lie at the intersection of machine learning, computer vision, natural language processing, and AI. He is a recipient of the Presidential Early Career Award for Scientists and Engineers (PECASE) (2019), the Early Career Award for Scientists and Engineers by the US Army (ECASE-Army) (2018), the Office of Naval Research (ONR) Young Investigator Program (YIP) award (2017), the National Science Foundation (NSF) CAREER award (2014), Army Research Office (ARO) Young Investigator Program (YIP) award (2014), Outstanding Junior Faculty awards from Georgia Tech (2018) and Virginia Tech (2015), multiple research awards from industry (Google, Amazon, Facebook), Carnegie Mellon Dean's Fellowship (2007), several best paper awards and nominations (ICLR 2023, CVPR 2022, ICCV 2019, EMNLP 2017) and teaching commendations.</p>
                    </div>
                </div>
                <p></p>

                <div class="row wow "  data-animation-delay="200">
                    <div class="col-sm-3 wow"  data-animation-delay="200">
                        <center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/invited/angel.png" alt=""></center>
                        <h4 class="section-heading"><center><a href="https://angelxuanchang.github.io/" target="blank">Angel Chang</a></center></h4>
                    </div>
                    <div class="col-sm-8 col-md-offset-1 wow" style="text-align:justify" data-animation-delay="200">
                        <p>Dr. Angel Chang is an Assistant Professor at Simon Fraser University. Prior to this, she was a visiting research scientist at Facebook AI Research and a research scientist at Eloquent Labs working on dialogue. She received my Ph.D. in Computer Science from Stanford, where she was part of the Natural Language Processing Group and advised by Chris Manning. Her research focuses on connecting language to 3D representations of shapes and scenes and grounding of language for embodied agents in indoor environments. She has worked on methods for synthesizing 3D scenes and shapes from natural language, and various datasets for 3D scene understanding. In general, she is interested in the semantics of shapes and scenes, the representation and acquisition of common sense knowledge, and reasoning using probabilistic models. </p>
                    </div>
                </div>
                <p></p>

                <div class="row wow "  data-animation-delay="200">
                    <div class="col-sm-3 wow"  data-animation-delay="200">
                        <center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/invited/daniel.jpg" alt=""></center>
                        <h4 class="section-heading"><center><a href="https://dpfried.github.io/" target="blank">Daniel Fried</a></center></h4>
                    </div>
                    <div class="col-sm-8 col-md-offset-1 wow" style="text-align:justify" data-animation-delay="200">
                        <p>Dr. Daniel Fried is an assistant professor at the Language Technologies Institute in the School of Computer Science at Carnegie Mellon University, working on natural language processing. His work focuses on enabling people to use language to interact with computers to carry out useful tasks in the world. One recurring theme in his work is pragmatics: viewing language as an action that people take in context to affect their communicative partners. He also studies domains where computers can complement human abilities. Recently, he has been focusing on code generation, aiming to make programming more communicative.</p>
                    </div>
                </div>
                <p></p>

                <div class="row wow "  data-animation-delay="200">
                    <div class="col-sm-3 wow"  data-animation-delay="200">
                        <center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/invited/heng.png" alt=""></center>
                        <h4 class="section-heading"><center><a href="https://blender.cs.illinois.edu/hengji.html" target="blank">Heng Ji </a></center></h4>
                    </div>
                    <div class="col-sm-8 col-md-offset-1 wow" style="text-align:justify" data-animation-delay="200">
                        <p>Dr. Heng Ji is a professor at Computer Science Department, and an affiliated faculty member at Electrical and Computer Engineering Department and Coordinated Science Laboratory of University of Illinois Urbana-Champaign. She is an Amazon Scholar. She is the Founding Director of Amazon-Illinois Center on AI for Interactive Conversational Experiences (AICE). Her research interests focus on Natural Language Processing, especially on Multimedia Multilingual Information Extraction, Knowledge-enhanced Large Language Models, Knowledge-driven Generation and Conversational AI. She was selected as a Young Scientist to attend the 6th World Laureates Association Forum, and selected to participate in DARPA AI Forward in 2023. </p>
                    </div>
                </div>
                <p></p>

                <div class="row wow "  data-animation-delay="200">
                    <div class="col-sm-3 wow"  data-animation-delay="200">
                        <center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/invited/alane.jpg" alt=""></center>
                        <h4 class="section-heading"><center><a href="https://www.alanesuhr.com/" target="blank">Alane Suhr</a></center></h4>
                    </div>
                    <div class="col-sm-8 col-md-offset-1 wow" style="text-align:justify" data-animation-delay="200">
                        <p>Dr. Alane Suhr is an Assistant Professor at UC Berkeley EECS. She received PhD in Computer Science at Cornell University, based at Cornell Tech in New York, NY, and advised by Yoav Artzi. Afterwards, she spent about a year in Seattle, WA at AI2 as a Young Investigator on the Mosaic team (led by Yejin Choi). Her research spans natural language processing, machine learning, and computer vision. She builds systems that use language to interact with people, e.g., in collaborative interactions (like CerealBar). She also designs models and datasets that address and represent problems in language grounding (e.g., NLVR), and develops learning algorithms for systems that learn language through interaction.</p>
                    </div>
                </div>
                <p></p>

            </div>
        </div>
    </div>
</div>

<!-- Organizers -->
<div id="organizers" class="content-section-b">

    <div class="container">
        <div class="row wow "  data-animation-delay="200">
            <h3 class="section-heading">Organizers</h3>

            <div class="col-sm-2 wow "  data-animation-delay="200">
                <center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/organizer/jing.png" alt=""></center>
                <h4 class="section-heading"><center><a href="https://g-jing.github.io/">Jing Gu</a></center></h4>
                <h5 class="section-heading"><center><i>UC Santa Cruz</i></center></h5>
            </div>

            <div class="col-sm-2 wow "  data-animation-delay="200">
                <center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/organizer/ray.jpg" alt=""></center>
                <h4 class="section-heading"><center><a href="https://tsujuifu.github.io/">Tsu-Jui (Ray) Fu </a></center></h4>
                <h5 class="section-heading"><center><i>UC Santa Barbara</i></center></h5>
            </div>

            <div class="col-sm-2 wow "  data-animation-delay="200">
                <center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/organizer/drew.jpg" alt=""></center>
                <h4 class="section-heading"><center><a href="https://cs.stanford.edu/people/dorarad/">Drew Hudson</a></center></h4>
                <h5 class="section-heading"><center><i>Google DeepMind</i></center></h5>
            </div>

            <div class="col-sm-2 wow "  data-animation-delay="200">
                <center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/organizer/asli.jpg" alt=""></center>
                <h4 class="section-heading"><center><a href="http://asli.us/">Asli Celikyilmaz </a></center></h4>
                <h5 class="section-heading"><center><i>Fundamentals AI Research (FAIR) @ Meta</i></center></h5>
            </div>
            <!-- </div> -->

            <!-- <div class="row wow "  data-animation-delay="200"> -->
            <div class="col-sm-2 wow "  data-animation-delay="200">
                <center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/organizer/william.jpg" alt=""></center>
                <h4 class="section-heading"><center><a href="https://sites.cs.ucsb.edu/~william/">William Wang</a></center></h4>
                <h5 class="section-heading"><center><i>UC Santa Barbara</i></center></h5>
            </div>

            <div class="col-sm-2 wow "  data-animation-delay="200">
                <center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/organizer/xin.jpg" alt=""></center>
                <h4 class="section-heading"><center><a href="https://eric-xw.github.io/">Xin Eric Wang</a></center></h4>
                <h5 class="section-heading"><center><i>UC Santa Cruz</i></center></h5>
            </div>        
        </div>
        <div class="wow " data-animation-delay="200">
          <h3 class="section-heading" style="color:white">Contact</h3>
          <!--div class="sub-title lead3">Lorem ipsum dolor sit atmet sit dolor greand fdanrh<br> sdfs sit atmet sit dolor greand fdanrh sdfs</div-->
          <p class="lead"  style="text-align:left;color:white">
          <!--Contact the Organizing Committee: alvr2024_naacl2024@softconf.com-->
          </p>

    </div>
    </div>
</div>

<!-- Program Committee -->

<div id ="committee" class="content-section-b" style="border-top: 0">
    <div class="container">
        <div class="row">

            <div class="col-sm-6 pull-right wow ">
                <img class="img-responsive " src="img/ipad.png" alt="">
            </div>

            <div class="wow " data-animation-delay="200">
                <h3 class="section-heading"> Program Committee</h3>

                <!--div class="sub-title lead3">Lorem ipsum dolor sit atmet sit dolor greand fdanrh<br> sdfs sit atmet sit dolor greand fdanrh sdfs</div-->
                <table cellspacing="0" cellpadding="0" style="margin-left: auto;margin-right: auto;width:75%">
                    <tr><td><li>Arjun Akula</td><td>Google</td></li><td></td></tr>
                    <tr><td><li>Asma Ben Abacha</td><td>Microsoft</td></li><td></td></tr>
                    <tr><td><li>Dhivya Chinnappa</td><td>Thomson Reuters</td></li><td></td></tr>
                    <tr><td><li>Simon Dobnik</td><td>University of Gothenburg</td></li><td></td></tr>
                    <tr><td><li>Thoudam Doren Singh</td><td>National Institute of Technology, Silchar, India</td></li><td></td></tr>
                    <tr><td><li>Zhe Gan</td><td>Apple AI/ML</td></li><td></td></tr>
                    <tr><td><li>Yulei Niu</td><td>Columbia University</td></li><td></td></tr>
                    <tr><td><li>Vikas Raunak</td><td>Microsoft</td></li><td></td></tr>
                    <tr><td><li>Arka Sadhu</td><td>University of Southern California</td></li><td></td></tr>
                    <tr><td><li>Loitongbam Sanayai Meetei</td><td>National Institute of Technology Silchar, India</td></li><td></td></tr>
                    <tr><td><li>Alok Singh</td><td>National Institute of Technology, Silchar India</td></li><td></td></tr>
                    <tr><td><li>Hao Tan</td><td>Adobe Research</td></li><td></td></tr>
                    <tr><td><li>Ece Takmaz</td><td>University of Amsterdam</td></li><td></td></tr>
                    <tr><td><li>Huaizu Jiang</td><td>Northeastern University</td></li><td></td></tr>
                    <tr><td><li>Cristina Garbacea</td><td>University of Michigan</td></li><td></td></tr>
                    <tr><td><li>Shubham Agarwal</td><td>Mila</td></li><td></td></tr>
                    <tr><td><li>Kaizhi Zheng</td><td>University of California, Santa Cruz</td></li><td></td></tr>
                    <tr><td><li>Yue Fan</td><td>University of California, Santa Cruz</td></li><td></td></tr>
                    <tr><td><li>Qianqi Yan</td><td>University of California, Santa Cruz</td></li><td></td></tr>
                    <tr><td><li>Wanrong Zhu</td><td>University of California, Santa Barbara</td></li><td></td></tr>
                    <tr><td><li>Yujie Lu</td><td>University of California, Santa Barbara</td></li><td></td></tr>
                </table>
                <br/>

                <!--p><a class="btn btn-embossed btn-primary" href="#" role="button">View Details</a>
                <a class="btn btn-embossed btn-info" href="#" role="button">Visit Website</a></p-->
            </div>
        </div>
    </div>

    <!-- /.container -->


    <!-- Sponsor -->
    <!-- <div id ="sponsors" class="content-section-a" style="border-top: 0"> -->
    <!-- <div class="container">			 -->
    <!-- <div class="row">			 -->
    <!--div class="col-sm-6 pull-right wow ">
        <img class="img-responsive " src="img/ipad.png" alt="">
    </div-->
    <!-- <div class="wow " data-animation-delay="200">    -->
    <!-- <h3 class="section-heading"> Sponsor </h3> -->
    <!--div class="sub-title lead3">Lorem ipsum dolor sit atmet sit dolor greand fdanrh<br> sdfs sit atmet sit dolor greand fdanrh sdfs</div-->
    <!-- <p class="lead"  style="text-align:justify"> -->
    <!-- <a href=" " target="_blank"><img src="img/sponsor/XXX.png" width="300"></a> -->
    <!-- </p> -->
    <!--p><a class="btn btn-embossed btn-primary" href="#" role="button">View Details</a>
    <a class="btn btn-embossed btn-info" href="#" role="button">Visit Website</a></p-->
    <!-- </div>    -->
    <!-- </div> -->
    <!-- </div> -->
    <!-- /.container -->
    <!-- </div> -->

    <!--
    <div id="contacts" class="content-section-c ">
        <div class="container">
            <div class="row">

                <div class="wow " data-animation-delay="200">
                    <h3 class="section-heading" style="color:white">Contact</h3>
                    <p class="lead"  style="text-align:left;color:white">
                        Contact the Organizing Committee: alvr2024_naacl2024@softconf.com
                    </p>

                </div>
            </div>
            <div class="row">

                <div class="col-md-6 col-md-offset-3 text-center">
                    <div >
                        <div class="morph-button ">
                            <button type="button"></button>

                        </div>
                    </div>
                </div>
            </div>

        </div>
    </div>
    -->

    <footer>

    </footer>

    <!-- JavaScript -->
    <script src="js/jquery-1.10.2.js"></script>
    <script src="js/bootstrap.min.js"></script>
    <script src="js/owl.carousel.js"></script>
    <script src="js/script.js"></script>
    <!-- StikyMenu -->
    <script src="js/stickUp.min.js"></script>
    <script type="text/javascript">
      jQuery(function($) {
        $(document).ready( function() {
          $('.navbar-default').stickUp();

        });
      });

    </script>
    <!-- Smoothscroll -->
    <script type="text/javascript" src="js/jquery.corner.js"></script>
    <script src="js/wow.min.js"></script>
    <script>
      new WOW().init();
    </script>
    <script src="js/classie.js"></script>
    <script src="js/uiMorphingButton_inflow.js"></script>
    <!-- Magnific Popup core JS file -->
    <script src="js/jquery.magnific-popup.js"></script>
</body>

</html>
