 <!-- FlatFy Theme - Andrea Galanti /-->
<!doctype html>
<!--[if lt IE 7]> <html class="no-js ie6 oldie" lang="en"> <![endif]-->
<!--[if IE 7]>    <html class="no-js ie7 oldie" lang="en"> <![endif]-->
<!--[if IE 8]>    <html class="no-js ie8 oldie" lang="en"> <![endif]-->
<!--[if IE 9]>    <html class="no-js ie9" lang="en"> <![endif]-->
<!--[if gt IE 9]><!--> <html> <!--<![endif]-->
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, user-scalable=no, initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0">
    <meta name="description" content="Workshop on Advances in Language and Vision Research ">
    <meta name="author" content="">

    <title>ALVR 2021</title>

    <!-- Bootstrap core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">
 
    <!-- Custom Google Web Font -->
    <link href="font-awesome/css/font-awesome.min.css" rel="stylesheet">
    <link href='http://fonts.googleapis.com/css?family=Lato:100,300,400,700,900,100italic,300italic,400italic,700italic,900italic' rel='stylesheet' type='text/css'>
	<link href='http://fonts.googleapis.com/css?family=Arvo:400,700' rel='stylesheet' type='text/css'>
	
    <!-- Custom CSS-->
    <link href="css/general.css" rel="stylesheet">
	
	 <!-- Owl-Carousel -->
    <link href="css/custom.css" rel="stylesheet">
	<link href="css/owl.carousel.css" rel="stylesheet">
    <link href="css/owl.theme.css" rel="stylesheet">
	<link href="css/style.css" rel="stylesheet">
	<link href="css/animate.css" rel="stylesheet">
	
	<!-- Magnific Popup core CSS file -->
	<link rel="stylesheet" href="css/magnific-popup.css"> 
	
	<script src="js/modernizr-2.8.3.min.js"></script>  <!-- Modernizr /-->
	<!--[if IE 9]>
		<script src="js/PIE_IE9.js"></script>
	<![endif]-->
	<!--[if lt IE 9]>
		<script src="js/PIE_IE678.js"></script>
	<![endif]-->

	<!--[if lt IE 9]>
		<script src="js/html5shiv.js"></script>
	<![endif]-->

</head>

<body id="home">

	<!-- Preloader -->
	<div id="preloader">
		<div id="status"></div>
	</div>
	
	<!-- FullScreen -->
    <div class="intro-header">
		<div class="col-xs-12 text-center abcen1">
			<h1 class="h1_home wow fadeIn" data-wow-delay="0.4s" style="text-shadow: 0 0 8px #000000;">2<sup>nd</sup> Workshop on Advances in Language and Vision Research (ALVR)</h1>
			<h3 class="h3_home wow fadeIn" data-wow-delay="0.6s" style="text-shadow: 0px 8px 4px black, 0 0 25px black"> 
				<!--In conjunction with <a style="color:white" href="https://acl2020.org/" target="_blank"><b>ACL 2020</b></a> </p> 
                July 9<sup>th</sup> 2020 (Full Day)</p>
                Location: Virtual</p>-->
                TBD</p>
			</h3>
			<!--ul class="list-inline intro-social-buttons">
				<li><a href="https://twitter.com/galantiandrea" class="btn  btn-lg mybutton_cyano wow fadeIn" data-wow-delay="0.8s"><span class="network-name">Twitter</span></a>
				</li>
				<li id="download" ><a href="#downloadlink" class="btn  btn-lg mybutton_standard wow swing wow fadeIn" data-wow-delay="1.2s"><span class="network-name">Free Download</span></a>
				</li>
			</ul-->
		</div>    
        <!-- /.container -->
		<div class="col-xs-12 text-center abcen wow fadeIn">
			<div class="button_down "> 
				<a class="imgcircle wow bounceInUp" data-wow-duration="1.5s"  href="#scope"> <img class="img_scroll" src="img/icon/circle.png" alt=""> </a>
			</div>
		</div>
    </div>
	
	<!-- NavBar-->
	<nav class="navbar-default" role="navigation">
		<div class="container">
			<div class="navbar-header">
				<button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
					<span class="sr-only">Toggle navigation</span>
					<span class="icon-bar"></span>
					<span class="icon-bar"></span>
					<span class="icon-bar"></span>
				</button>
				<a class="navbar-brand" href="#home">ALVR 2021</a>
			</div>

			<div class="collapse navbar-collapse navbar-right navbar-ex1-collapse">
				<ul class="nav navbar-nav">
					
					<li class="menuItem"><a href="#scope">Home</a></li>
					<li class="menuItem"><a href="#submit">Submission</a></li>
					<li class="menuItem"><a href="#proceedings">Proceedings</a></li>
					<li class="menuItem"><a href="#program">Program</a></li>
					<li class="menuItem"><a href="#invited">Invited Speakers</a></li>
					<li class="menuItem"><a href="#organizers">Organizers</a></li>	
					<!-- <li class="menuItem"><a href="#sponsors">Sponsor</a></li>					 -->
					<li class="menuItem"><a href="#contacts">Contact</a></li>
				</ul>
			</div>
		   
		</div>
	</nav> 
	
	<!-- Scope -->
    <div id ="scope" class="content-section-a" style="border-top: 0">

        <div class="container">
			
            <div class="row">
			
				<!--div class="col-sm-6 pull-right wow fadeInRightBig">
                    <img class="img-responsive " src="img/ipad.png" alt="">
                </div-->
		<!--<img  class="img-responsive"  style="width: 20vw"  src="img/cvpr_logo.png" alt="">
		<img  class="img-responsive"  style="width: 20vw"  src="img/cvf.jpg" alt="">
		<img  class="img-responsive"  style="width: 10vw"  src="img/organizer/vgis.png" alt="">  -->
                <div class="wow fadeInRightBig" data-animation-delay="200"> 
                    <!--<h3 class="section-heading">ACL 2020 Workshop on Advances in Language and Vision Research</h3>-->
                    <h3 class="section-heading">Workshop on Advances in Language and Vision Research</h3>
		    
					<!--div class="sub-title lead3">Lorem ipsum dolor sit atmet sit dolor greand fdanrh<br> sdfs sit atmet sit dolor greand fdanrh sdfs</div-->
                    <p class="lead"  style="text-align:justify">
						Language and vision research has attracted great attention from both natural language processing (NLP) and computer vision (CV) researchers. Gradually, this area is shifting from passive perception, templated language, and synthetic imagery/environments to active perception, natural language, and photo-realistic simulation or real world deployment. Thus far, few workshops on language and vision research have been organized by groups from the NLP community. We propose <b>the first workshop on Advances in Language and Vision Research (ALVR)</b> in order to promote the frontier of language and vision research and to bring interested researchers together to discuss how to best tackle and solve real-world problems in this area.<br/><br/>
This workshop covers (but is not limited to) the following topics: <br/>
						<ul class="lead">
							<li> New tasks and datasets that provide real-world solutions in the intersection of NLP and CV; </li>
							<li> Language-guided interaction with the real world, such as navigation via instruction following or dialogue; </li>
							<li> External knowledge integration in visual and language understanding; </li>
							<li> Visually grounded multilingual study, for example multimodal machine translation; </li>
							<li> Shortcoming of existing language and vision tasks and datasets; </li>
							<li> Benefits of using multimodal learning in downstream NLP tasks; </li>
							<li> Self-supervised representation learning in language and vision; </li>
							<li> Transfer learning (including few/zero-shot learning) and domain adaptation; </li>
							<li> Cross-modal learning beyond image understanding, such as videos and audios; </li>
							<li> Multidisciplinary study that may involve linguistics, cognitive science, robotics, etc. </li>
						</ul>
					</p>
					

					<h3 id="vmt" class="anchor" aria-hidden="True" href="#vmt">Video-guided Machine Translation (VMT) Challenge</h3>

<p class="lead"  style="text-align:justify">We also hold the first Video-guided Machine Translation (VMT) Challenge. This challenge aims to benchmark progress towards models that translate source language sentence into the target language with video information as the additional spatiotemporal context. The challenge is based on the recently released large-scale multilingual video description dataset, VATEX. The VATEX dataset contains over 41,250 videos and 825,000 high-quality captions in both English and Chinese, half of which are English-Chinese translation pairs.

<p class="lead"  style="text-align:justify">Winners will be announced and awarded in the workshop. </p>

<p class="lead"  style="text-align:justify">Please refer to the <a href="https://eric-xw.github.io/vatex-website/translation_2020.html" style="color:orange">VMT Challenge website</a> for additional details (such as participation, dates, and starter code)!</p>

<h3 id="reverie" class="anchor" aria-hidden="True" href="#reverie">REVERIE Challenge</h3>
<p class="lead"  style="text-align:justify">The objective of REVERIE Challenge is to benchmark the state-of-the-art for the remote object grounding task defined in our CVPR <a href="https://arxiv.org/abs/1904.10151">paper</a>, in the hope that it might drive progress towards more flexible and powerful human interactions with robots.</p >
<p class="lead"  style="text-align:justify">The REVERIE task requires an intelligent agent to correctly localise a remote target object (cannot be observed at the starting location) specified by a concise high-level natural language instruction, such as 'bring me the blue cushion from the sofa in the living room'. Since the target object is in a different room from the starting one, the agent needs first to navigate to the goal location. When the agent determines to stop, it should select one object from a list of candidates provided by the simulator. The  agent can attempt to localise the target at any step, which is totally up to algorithm design.   But we only allow the agent output once in each episode, which means the agent only can guess the answer once in a single run. </p >
<p class="lead"  style="text-align:justify">Please visit <a href="https://yuankaiqi.github.io/REVERIE_Challenge" style="color:orange">REVERIE Challenge website</a > for more details!</p >

<a id="important-dates" class="anchor" href="#accepted-papers" aria-hidden="true"><span class="octicon octicon-link"></span></a>


					 <!--p><a class="btn btn-embossed btn-primary" href="#" role="button">View Details</a> 
					 <a class="btn btn-embossed btn-info" href="#" role="button">Visit Website</a></p-->
				</div>   
            </div>
        </div>
        <!-- /.container -->
    </div>

	<!-- Call for Papers -->
    <div id ="submit" class="content-section-b">

        <div class="container">
			
            <div class="row">
			
				<!--div class="col-sm-6 pull-right wow fadeInRightBig">
                    <img class="img-responsive " src="img/ipad.png" alt="">
                </div-->
				
                <div class="wow fadeInLeftBig" data-animation-delay="200">   
                    <h3 class="section-heading">Submission</h3>
					<!--div class="sub-title lead3">Lorem ipsum dolor sit atmet sit dolor greand fdanrh<br> sdfs sit atmet sit dolor greand fdanrh sdfs</div-->

                        <p class="lead"  style="text-align:justify">The workshop includes an archival and a non-archival track on topics related to language-and-vision research. For both tracks, the reviewing process is single-blind. That is, the reviewer will know the authors but not the other way around. Submission is electronic, using the Softconf START conference management system. The submission site will be available at TBD.<!--<a href="https://www.softconf.com/acl2020/alvr" style="color:orange">https://www.softconf.com/acl2020/alvr</a>.--></p>
                        <p class="lead"  style="text-align:justify">If you are interested in taking a more active part in the workshop, we also encourage you to apply to join the program committee and participate in reviewing submissions following this link: TBD.<!--<a href="https://forms.gle/voyxjQLFb8duYM5e7">https://forms.gle/voyxjQLFb8duYM5e7</a>.--> Qualified reviewers will be selected based on the prior reviewing experience and publication records.</p>
                        
                        <h4 class="section-heading">Archival Track</h4>
                        <p class="lead"  style="text-align:justify">The archival track follows the ACL short paper format. Submissions to the archival track may consist of up to 4 pages of content (excluding references) in ACL format (style sheets are available below), plus unlimited references. Accepted papers will be given 5 content pages for camera-ready version. Authors are encouraged to use this additional page to address reviewers’ comments in their final versions. <!--The accepted papers to the archival track will be included in the ACL 2020 Workshop proceedings.--> The archival track does not accept double submissions, e.g., no previously published papers or concurrent submissions to other conferences or workshops.</p>
                        <p class="lead"  style="text-align:justify">The format of submitted papers to the archival track must follow the ACL Author Guidelines. Style sheets (Latex, Word) are available <a href="http://acl2020.org/downloads/acl2020-templates.zip" target="_blank">here</a>. And the Overleaf template is also available <a href="https://www.overleaf.com/latex/templates/acl-2020-proceedings-template/zsrkcwjptpcd" target="_blank">here</a>.</p>
                        
                        <h4 class="section-heading">Non-archival Track</h4>
                        <p class="lead"  style="text-align:justify">The workshop also includes a non-archival track to allow submission of previously published papers and double submission to ALVR and other conferences or journals. Accepted non-archival papers can still be presented as posters at the workshop.</p>
                        <p class="lead"  style="text-align:justify">There are no formatting or page restrictions for non-archival submissions. The accepted papers to the non-archival track will be displayed on the workshop website.<!--, but will NOT be included in the ACL 2020 Workshop proceedings or otherwise archived.--></p>
				</div>  

				<div class="wow fadeInRightBig" data-animation-delay="200">   
                    <h3 class="section-heading">Important Dates</h3>
					<!--div class="sub-title lead3">Lorem ipsum dolor sit atmet sit dolor greand fdanrh<br> sdfs sit atmet sit dolor greand fdanrh sdfs</div-->
                    <ul class="lead">
					  <!--<del><li><b>Submission Deadline (extended): April 16, 2020</b> (11:59pm Anywhere on Earth time, UTC-12)</li></del>
					  <del><li>Notification: May 11, 2020</li></del>
					  <del><li>Camera Ready deadline: May 21, 2020 (11:59pm Anywhere on Earth time, UTC-12)</li></del>
                      <li>Workshop Day: July 9, 2020</li>-->
                      <li>TBD</li>
					</ul>
					
					</ul>					
				</div>  				
            </div>
        </div>
        <!-- /.container -->
    </div>
	 <div id ="proceedings" class="content-section-b">

        <div class="container">

            <div class="row">

				<!--div class="col-sm-6 pull-right wow fadeInRightBig">
                    <img class="img-responsive " src="img/ipad.png" alt="">
                </div-->

                <div class="wow fadeInLeftBig" data-animation-delay="200">
                    <h3 class="section-heading">Proceedings</h3>
                    <!--<h4 class="lead">Papers accepted to the archival track:</h4>
                    <ul>
                        <li>Extending ImageNet to Arabic using Arabic WordNet - <em>Abdulkareem Alsudais</em> (<a href="https://alvr-workshop.github.io/proceedings/ALVR_2020_02_Paper.pdf" style="color:orange">PDF</a> | <a href="https://slideslive.com/38929757/extending-imagenet-to-arabic-using-arabic-wordnet" style="color:orange">Recorded Talk</a>)</li>
                        <li>Toward General Scene Graph: Integration of Visual Semantic Knowledge with Entity Synset Alignment - <em>Woo Suk Choi, Kyoung-Woon On, Yu-Jung Heo and Byoung-Tak Zhang</em> (<a href="https://alvr-workshop.github.io/proceedings/ALVR_2020_06_Paper.pdf" style="color:orange">PDF</a> | <a href="https://slideslive.com/38929758/toward-general-scene-graph-integration-of-visual-semantic-knowledge-with-entity-synset-alignment" style="color:orange">Recorded Talk</a>)</li>
                        <li>Visual Question Generation from Radiology Images - <em>Mourad Sarrouti, Asma Ben Abacha and Dina Demner-Fushman</em>(<a href="https://alvr-workshop.github.io/proceedings/ALVR_2020_15_Paper.pdf" style="color:orange">PDF</a> | <a href="https://slideslive.com/38929760/visual-question-generation-from-radiology-images" style="color:orange">Recorded Talk</a> | <a href="https://drive.google.com/drive/folders/13kJ46Rl7CN608M8mF2014S8D3kkcBYcg?usp=sharing" style="color:orange">Slides & Video</a>)</li>
                        <li>On the role of effective and referring questions in GuessWhat?! - <em>Mauricio Mazuecos, Alberto Testoni, Raffaella Bernardi and Luciana Benotti</em>(<a href="https://alvr-workshop.github.io/proceedings/ALVR_2020_16_Paper.pdf" style="color:orange">PDF</a> | <a href="https://slideslive.com/38929756/on-the-role-of-effective-and-referring-questions-in-guesswhat" style="color:orange">Recorded Talk</a>)</li>
                        <li>Latent Alignment of Procedural Concepts in Multimodal Recipes - <em>Hossein Rajaby Faghihi, Roshanak Mirzaee, Sudarshan Paliwal and Parisa Kordjamshidi</em>(<a href="https://alvr-workshop.github.io/proceedings/ALVR_2020_22_Paper.pdf" style="color:orange">PDF</a> | <a href="https://slideslive.com/38929759/latent-alignment-of-procedural-concepts-in-recipes" style="color:orange">Recorded Talk</a>)</li>
                    </ul>
                    <h4 class="lead"><br />Papers accepted to the non-archival track:</h4>
                    <ul>
                        <li>Pix2R: Guiding Reinforcement Learning using Natural Language by Mapping Pixels to Rewards - <em>Prasoon Goyal, Scott Niekum and Raymond Mooney</em> (<a href="https://alvr-workshop.github.io/proceedings/ALVR_2020_05_Paper.pdf" style="color:orange">PDF</a> | <a href="https://youtu.be/uUAAT00qFr4" style="color:orange">Recorded Talk</a>)</li>
                        <li>TextCaps: a Dataset for Image Captioning with Reading Comprehension - <em>Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach and Amanpreet Singh</em> (<a href="https://alvr-workshop.github.io/proceedings/ALVR_2020_08_Paper.pdf" style="color:orange">PDF</a> | <a href="https://www.dropbox.com/s/jvqkm3px25wvc7e/textcaps_abstract_sidorov.mp4?dl=0 " style="color:orange">Recorded Talk</a>)</li>
                        <li>Improving VQA and its Explanations by Comparing Competing Explanations - <em>Jialin Wu, Liyan Chen and Raymond Mooney</em> (<a href="https://alvr-workshop.github.io/proceedings/ALVR_2020_09_Paper.pdf" style="color:orange">PDF</a>)</li>
                        <li>Bridging Languages through Images with Deep Partial Canonical Correlation Analysis - <em>Guy Rotman, Ivan Vulić and Roi Reichart</em> (<a href="https://alvr-workshop.github.io/proceedings/ALVR_2020_10_Paper.pdf" style="color:orange">PDF</a>)</li>
                        <li>Counterfactual Vision-and-Language Navigation via Adversarial Path Sampling - <em>Tsu-Jui Fu, Xin Wang, Matthew Peterson, Scott Grafton, Miguel Eckstein and William Yang Wang</em> (<a href="https://alvr-workshop.github.io/proceedings/ALVR_2020_12_Paper.pdf" style="color:orange">PDF</a> | <a href="https://youtu.be/eCPtNWDe2RQ" style="color:orange">Recorded Talk</a> | <a href="https://alvr-workshop.github.io/paper_materials/aps.pdf" style="color:orange">Slides</a>)</li>
                        <li>Measuring Social Biases in Grounded Vision and Language Embeddings - <em>Candace Ross, Boris Katz and Andrei Barbu</em> (<a href="https://alvr-workshop.github.io/proceedings/ALVR_2020_14_Paper.pdf" style="color:orange">PDF</a>)</li>
                        <li>Exploring Phrase Grounding without Training: Contextualisation and Extension to Text-Based Image Retrieval - <em>Letitia Parcalabescu and Anette Frank</em> (<a href="https://alvr-workshop.github.io/proceedings/ALVR_2020_17_Paper.pdf" style="color:orange">PDF</a> | <a href="https://1drv.ms/u/s!AiWgnG7J0DD-gdI5n7Me1ontbY_vLA?e=f5bilJ" style="color:orange">Recorded Talk</a>)</li>
                        <li>What is Learned in Visually Grounded Neural Syntax Acquisition - <em>Noriyuki Kojima, Hadar Averbuch-Elor, Alexander Rush and Yoav Artzi</em> (<a href="https://alvr-workshop.github.io/proceedings/ALVR_2020_18_Paper.pdf" style="color:orange">PDF</a> | <a href="https://www.dropbox.com/s/dx1ecbvdsyvd0cl/Presentation.mov?dl=0" style="color:orange">Recorded Talk</a>)</li>
                        <li>Learning to Map Natural Language Instructions to Physical Quadcopter Control Using Simulated Flight - <em>Valts Blukis, Yannick Terme, Eyvind Niklasson, Ross Knepper and Yoav Artzi</em> (<a href="https://alvr-workshop.github.io/proceedings/ALVR_2020_19_Paper.pdf" style="color:orange">PDF</a> | <a href="https://www.youtube.com/watch?v=O7G0HYGqU4w" style="color:orange">Demo Video</a> | <a href="https://www.cs.cornell.edu/~valts/docs/corl19_poster.pdf" style="color:orange">Poster</a>)</li>
                        <li>Learning Latent Graph Representations for Relational VQA - <em>Liyan Chen and Raymond Mooney</em> (<a href="https://alvr-workshop.github.io/proceedings/ALVR_2020_20_Paper.pdf" style="color:orange">PDF</a>)</li>
                        <li>Entity Skeletons for Visual Storytelling - <em>Khyathi Raghavi Chandu, Ruo-Ping Dong and Alan W Black</em> (<a href="https://alvr-workshop.github.io/proceedings/ALVR_2020_21_Paper.pdf" style="color:orange">PDF</a>)</li>
                        <li>Keyframe Segmentation and Positional Encoding for Video-guided Machine Translation Challenge 2020 - <em>Tosho Hirasawa, Zhishen Yang, Mamoru Komachi, Naoaki Okazaki</em> (<a href="https://arxiv.org/abs/2006.12799" style="color:orange">PDF</a> | <a href="https://youtu.be/zHwXPmIQajA" style="color:orange">Recorded Talk</a>)</li>
                        <li>DeepFuse: HKU’s Multimodal Machine Translation System for VMT’20 - <em>Zhiyong Wu</em> (<a href="./challenge_papers/DeepFuse.pdf" style="color:orange">PDF</a> | <a href="http://www.youtube.com/watch?v=zHwXPmIQajA&t=8m37s" style="color:orange">Recorded Talk</a>)</li>
                        <li>Enhancing Neural Machine Translation with Multimodal Rewards - <em>Yuqing Song, Shizhe Chen, Qin Jin</em> (<a href="./challenge_papers/ruc_report.pdf" style="color:orange">PDF</a> | <a href="http://www.youtube.com/watch?v=zHwXPmIQajA&t=16m16s" style="color:orange">Recorded Talk</a>)</li>
                    </ul>-->
                    <li>TBD</li>
				</div>
			</div>
		</div>
	 </div>
	<!-- Program -->
    <div id ="program" class="content-section-a">

        <div class="container">				
                <div class="wow fadeInLeftBig" data-animation-delay="200">   
                    <h3 class="section-heading">Program (PDT)</h3>

                    <!--<p class="lead">
                    	<b>ATTENTION! The video recording of the whole workshop is located at <a href="https://slideslive.com/38931798/w9-alvr-live-stream" style="color:orange"> <u>https://slideslive.com/38931798/w9-alvr-live-stream</u></a></b>. Some pre-recorded talks are listed below, while others are included in the workshop recording.
					</p>-->
				</div>
		    
  <table class="table" style="width:100%">
      <tr class="info">
        <td>8:20-8:25</td>
        <td><strong>Opening Remarks [<a href="./talks/ALVR_opening.pdf" style="color:orange">slides</a>]</strong></td>
        <td>Workshop Organizers</td>
      </tr>

      <tr class="success">
        <td>8:25-9:10</td>
        <td><strong>Grounding Natural Language to 3D [<a href="./talks/Angel_ALVR.pdf" style="color:orange">slides</a>]</strong><br />Invited Talk & QA</td>
        <td>Angel Chang</td>
      </tr>

      <tr class="success">
        <td>9:10-9:55</td>
        <td><strong>Challenges in Evaluating Vision and Language Tasks</strong><br />Invited Talk & QA</td>
        <td>Lucia Specia</td>
      </tr>

      <tr class="success">
        <td>9:55-10:40</td>
        <td><strong>Multimodal AI: Understanding Human Behaviors [<a href="./talks/Morency-MultimodalAI.pdf" style="color:orange">slides</a>]</strong><br />Invited Talk & QA</td>
        <td>Louis-Philippe Morency</td>
      </tr>

      <tr>
        <td></td>
        <td><strong>Break</strong></td>
        <td> </td>
      </tr>

      <tr class="success">
        <td>10:50-11:35</td>
        <td><strong>Robot Control in Situated Instruction Following [<a href="https://slideslive.com/38929761/robot-control-in-situated-instruction-following" style="color:orange">video</a>] [<a href="./talks/yoav-control-alvr.pdf" style="color:orange">slides</a>]</strong><br />Invited Talk & QA</td>
        <td>Yoav Artzi</td>
      </tr>

      <tr class="info">
        <td>11:35-11:45</td>
        <td><strong>Video-guided Machine Translation (VMT) Challenge [<a href="./talks/VMT_Challenge.pdf" style="color:orange">slides</a>]</strong></td>
        <td>Xin Wang</td>
      </tr>

      <tr class="info">
        <td>11:45-12:10</td>
        <td><strong>VMT Challenge Talk: </strong><br>
         <li>Keyframe Segmentation and Positional Encoding for Video-guided Machine Translation [<a href="https://youtu.be/zHwXPmIQajA" style="color:orange">video</a>]</li>
         <li>DeepFuse: HKU’s Multimodal Machine Translation System for VMT’20 [<a href="http://www.youtube.com/watch?v=zHwXPmIQajA&t=8m37s" style="color:orange">video</a>]</li></li>
         <li>Enhancing Neural Machine Translation with Multimodal Rewards [<a href="http://www.youtube.com/watch?v=zHwXPmIQajA&t=16m16s" style="color:orange">video</a>]</li></li>
         </td>
	      <td><br>Tosho Hirasawa <i>et al.</i>
            <br>Zhiyong Wu 
            <br>Yuqing Song <i>et al.</i>
        </td>
      </tr>


      <tr class="info">
        <td>12:10-12:20</td>
        <td><strong>VMT Challenge Live QA</strong></td>
        <td>All the Challenge Presenters</td>
      </tr>

      <tr>
        <td></td>
        <td><strong>Break</strong></td>
        <td> </td>
      </tr>

      <tr class="success">
        <td>13:30-14:15</td>
        <td><strong>Augment Machine Intelligence with Multimodal Information [<a href="https://slideslive.com/38929766/agument-intelligence-with-multimodal-information" style="color:orange">video</a>]</li></strong><br />Invited Talk & QA</td>
        <td>Zhou Yu</td>
      </tr>

      <tr class="success">
        <td>14:15-15:00</td>
        <td><strong>Dungeons and DQNs: Grounding Language in Shared Experience [<a href="https://slideslive.com/38929764/dungeons-and-dqns-grounding-language-in-shared-experience" style="color:orange">video (SlidesLive)</a>] [<a href="https://youtu.be/YemciyRtYeI" style="color:orange">video (YouTube)</a>]</li></strong><br />Invited Talk & QA</td>
        <td>Mark Riedl</td>
      </tr>

      <tr class="info">
        <td>15:00-15:15</td>
        <td><strong>REVERIE Challenge [<a href="https://youtu.be/X__8sujXZbo" style="color:orange">video</a>]</li></strong></td>
        <td>Yuankai Qi</td>
      </tr>

      <tr class="info">
        <td>15:15-15:35</td>
        <td><strong>REVERIE Challenge Winner Talk:</strong><br>
        Distance-aware and Robust Network with Wandering Reducing Strategy for REVERIE [<a href="https://youtu.be/g3pMFzCkP-k" style="color:orange">video</a>]</li></td>
	      <td><br>Chen Gao <i>et al.</i> <!-- Jinyu Chen, Erli Meng, <br>Liang Shi, Xiao Lu, Si Liu --></td>
      </tr>

      <tr class="info">
        <td>15:35-15:45</td>
        <td><strong>REVERIE Challenge Live QA</strong></td>
        <td>All the Challenge Presenters</td>
      </tr>

      <tr>
        <td></td>
        <td><strong>Break</strong></td>
        <td></td>
      </tr>

      <tr class="success">
        <td>16:00-16:45</td>
        <td><strong>Vision+Language Research: Self-supervised Learning, Adversarial Training, <br>Multimodal Inference and Explainability [<a href="https://slideslive.com/38929762/multimodal-ai" style="color:orange">video</a>] [<a href="./talks/ACL_2020_ALVR_JJ_Liu.pdf" style="color:orange">slides</a>]</li></strong><br />Invited Talk & QA</td>
        <td>Jingjing Liu</td>
      </tr>

      <tr class="info">
        <td>16:45-17:10</td>
        <td><strong>Archival Track Recorded Talks [<a href="#proceedings" style="color:orange">link</a>]</strong>
        </td>
        <td></td>
      </tr>

      <tr class="info">
        <td>17:10-17:45</td>
        <td><strong>Poster Session and QA</strong></td>
        <td>All the Workshop Paper Authors</td>
      </tr>
    </table>

		    
		    

            <div class="container">				
                <div class="wow fadeInRightBig" data-animation-delay="200">
                </div>
            </div>
        </div>
    </div>
	<!-- Invited Speakers -->
    <div id ="invited" class="content-section-b">

        <div class="container">
			
            <div class="row">
				<div class="container">

                <div class="row wow fadeInRightBig"  data-animation-delay="200">
					<h3 class="section-heading">Invited Speakers</h3>
                    <!--<p>(presentation order)</p>-->
                    <br>
                </div>

                <div class="row wow fadeInRightBig"  data-animation-delay="200">
                    <div class="col-sm-3 wow"  data-animation-delay="200">   
                        <center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/invited/jacob.jpg" alt=""></center>
                        <h4 class="section-heading"><center><a href="https://www.mit.edu/~jda/" target="blank">Jacob Andreas</a></center></h4>
                    </div>
                    <div class="col-sm-8 col-md-offset-1 wow" style="text-align:justify" data-animation-delay="200">   
                        <p>Jacob Andreas is the X Consortium Career Development Assistant Professor at MIT. His research focuses on building intelligent systems that can communicate effectively using language and learn from human guidance. Jacob earned his Ph.D. from UC Berkeley, his M.Phil. from Cambridge (where he studied as a Churchill scholar) and his B.S. from Columbia. He has been the recipient of an NSF graduate fellowship, a Facebook fellowship, and paper awards at NAACL and ICML.</p>
                    </div>
                </div>
                <p></p>

                <div class="row wow fadeInRightBig"  data-animation-delay="200">
                    <div class="col-sm-3 wow"  data-animation-delay="200">   
                        <center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/invited/jason.jpg" alt=""></center>
                        <h4 class="section-heading"><center><a href="http://www.jasonbaldridge.com/" target="blank">Jason Baldridge</a></center></h4>
                    </div>
                    <div class="col-sm-8 col-md-offset-1 wow" style="text-align:justify" data-animation-delay="200">   
                        <p>Jason Baldridge is a research scientist at Google working on grounded language understanding. Co-founder of People Pattern. He was previously an Associate Professor in the Department of Linguistics at the University of Texas at Austin.</p>
                    </div>
                </div>
                <p></p>

                <div class="row wow fadeInRightBig"  data-animation-delay="200">
                    <div class="col-sm-3 wow"  data-animation-delay="200">   
                        <center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/invited/mohit.png" alt=""></center>
                        <h4 class="section-heading"><center><a href="https://www.cs.unc.edu/~mbansal/" target="blank">Mohit Bansal</a></center></h4>
                    </div>
                    <div class="col-sm-8 col-md-offset-1 wow" style="text-align:justify" data-animation-delay="200">   
                        <p>Dr. Mohit Bansal is the John R. & Louise S. Parker Associate Professor and the Director of the MURGe-Lab (UNC-NLP Group) in the Computer Science department at University of North Carolina (UNC) Chapel Hill. He received his PhD from UC Berkeley in 2013 (where he was advised by Dan Klein) and his BTech from IIT Kanpur in 2008. His research expertise is in statistical natural language processing and machine learning, with a particular focus on multimodal, grounded, and embodied semantics (i.e., language with vision and speech, for robotics), human-like language generation and Q&A/dialogue, and interpretable and generalizable deep learning.</p>
                    </div>
                </div>
                <p></p>

                <div class="row wow fadeInRightBig"  data-animation-delay="200">
                    <div class="col-sm-3 wow"  data-animation-delay="200">   
                        <center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/invited/yonatan.jpg" alt=""></center>
                        <h4 class="section-heading"><center><a href="https://yonatanbisk.com/" target="blank">Yonatan Bisk</a></center></h4>
                    </div>
                    <div class="col-sm-8 col-md-offset-1 wow" style="text-align:justify" data-animation-delay="200">   
                        <p>Yonatan Bisk is an Assistant Professor at CMU. His research area is Natural Language Processing (NLP) with a focus on grounding. In particular, his work broadly falls into: 1. Uncovering the latent structures of natural language, 2. Modeling the semantics of the physical world, and 3. Connecting language to perception and control.</p>
                    </div>
                </div>
                <p></p>

                <div class="row wow fadeInRightBig"  data-animation-delay="200">
                    <div class="col-sm-3 wow"  data-animation-delay="200">   
                        <center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/invited/joyce.jpg" alt=""></center>
                        <h4 class="section-heading"><center><a href="https://web.eecs.umich.edu/~chaijy/" target="blank">Joyce Y. Chaijoyce</a></center></h4>
                    </div>
                    <div class="col-sm-8 col-md-offset-1 wow" style="text-align:justify" data-animation-delay="200">   
                        <p>Joyce Y. Chaijoyce is a Professor at University of Michigan. Her research interests are in the area of artificial intelligence, particularly on natural language processing, situated dialogue agents, human-robot communication, and intelligent user interfaces. Her recent work has focused on grounded language processing to facilitate situated communication with robots and other artificial agents. Prior to joining UM, she was a professor at MSU directing the Language and Interaction Research Lab . At UM, she is a member of Michigan AI Lab and directing the Situated Language and Embodied Dialogue (SLED) research group. She is also affiliated with Michigan Robotics Institute.</p>
                    </div>
                </div>
                <p></p>

           	 </div>
        	</div>
    	</div>
	</div>
	
	<!-- Organizers -->
    <div id="organizers" class="content-section-a"> 
		
		<div class="container">
            <div class="row wow fadeInRightBig"  data-animation-delay="200">
                <h3 class="section-heading">Organizers</h3>
            
                <div class="col-sm-2 wow fadeInRightBig"  data-animation-delay="200">   
                    <center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/organizer/xin.jpg" alt=""></center>
                    <h4 class="section-heading"><center><a href="https://eric-xw.github.io/">Xin (Eric) Wang</a></center></h4>
                    <h5 class="section-heading"><center><i>UC Santa Cruz</i></center></h5>
                </div>

                <div class="col-sm-2 wow fadeInRightBig"  data-animation-delay="200">   
                    <center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/organizer/ronghang.png" alt=""></center>
                    <h4 class="section-heading"><center><a href="http://ronghanghu.com/">Ronghang Hu</a></center></h4>
                    <h5 class="section-heading"><center><i>UC Berkeley</i></center></h5>
                </div>
            
                <div class="col-sm-2 wow fadeInRightBig"  data-animation-delay="200">   
                    <center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/organizer/drew.jpg" alt=""></center>
                    <h4 class="section-heading"><center><a href="https://scholar.google.com/citations?user=q_trRV0AAAAJ">Drew Hudson</a></center></h4>
                    <h5 class="section-heading"><center><i>Stanford</i></center></h5>
                </div>

                <div class="col-sm-2 wow fadeInRightBig"  data-animation-delay="200">   
                    <center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/organizer/ray.jpg" alt=""></center>
                    <h4 class="section-heading"><center><a href="https://tsujuifu.github.io/">Tsu-Jui Fu</a></center></h4>
                    <h5 class="section-heading"><center><i>UC Santa Barbara</i></center></h5>
                </div>
            </div>

            <div class="row wow fadeInRightBig"  data-animation-delay="200">
                <div class="col-sm-2 wow fadeInRightBig"  data-animation-delay="200">   
                    <center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/organizer/marcus.jpg" alt=""></center>
                    <h4 class="section-heading"><center><a href="https://rohrbach.vision/">Marcus Rohrbach</a></center></h4>
                    <h5 class="section-heading"><center><i>FAIR</i></center></h5>
                </div>

                <div class="col-sm-2 wow fadeInRightBig"  data-animation-delay="200">   
                    <center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/organizer/daniel.jpg" alt=""></center>
                    <h4 class="section-heading"><center><a href="http://people.eecs.berkeley.edu/~dfried/">Daniel Fried</a></center></h4>
                    <h5 class="section-heading"><center><i>UC Berkeley</i></center></h5>
                </div>
            </div>
        </div>
    </div>

	<!-- Program Committee -->

    <div id ="committee" class="content-section-b" style="border-top: 0">
        <div class="container">
            <div class="row">

				<div class="col-sm-6 pull-right wow fadeInRightBig">
                    <img class="img-responsive " src="img/ipad.png" alt="">
                </div>

                <div class="wow fadeInLeftBig" data-animation-delay="200">
                    <h3 class="section-heading"> Program Committee</h3>

					<!--div class="sub-title lead3">Lorem ipsum dolor sit atmet sit dolor greand fdanrh<br> sdfs sit atmet sit dolor greand fdanrh sdfs</div-->
<table cellspacing="0" cellpadding="0" style="margin-left: auto;margin-right: auto;width:75%">
					<tr><td><li>Jacob Andreas</td> <td>MIT</td></li> <td> </td></tr>
					<tr><td><li>Angel Chang</td><td>Simon Fraser Univeristy</li></td> <td> </td></tr>
					<tr><td><li>Devendra Chaplot</td><td>CMU</li></td> <td> </td></tr>
					<tr><td><li>Abhishek Das</td><td>Georgia Tech</li></td> <td> </td></tr>
					<tr><td><li>Daniel Fried </td><td>UC Berkeley</li></td> <td> </td></tr>
					<tr><td><li>Zhe Gan</td><td>Microsoft</li></td> <td> </td></tr>
					<tr><td><li>Christopher Kanan</td><td>Rochester Institute of Technology</td></li> <td> </td></tr>
					<tr><td><li>Jiasen Lu</td><td>Georgia Tech</li></td> <td> </td></tr>
					<tr><td><li>Ray Mooney</td><td>University of Texas, Austin </td></li> <td> </td></tr>
					<tr><td><li>Khanh Nguyen</td><td>University of Maryland </td></li> <td> </td></tr>
					<tr><td><li>Aishwarya Padmakumar</td> <td>University of Texas, Austin </td></li> <td> </td></tr>
					<tr><td><li>Hamid Palangi</td> <td>Microsoft Research</td></li> <td> </td></tr>
        			<tr><td><li>Alessandro Suglia</td><td>Heriot-Watt University</td></li> <td> </td></tr>
					<tr><td><li>Vikas Raunak</td><td>CMU</td></li> <td> </td></tr>
					<tr><td><li>Volkan Cirik</td><td>CMU</td></li> <td> </td></tr>
					<tr><td><li>Parminder Bhatia</td><td>Amazon</td></li> <td> </td></tr>
					<tr><td><li>Khyathi Raghavi Chandu</td><td>CMU</td></li> <td> </td></tr>
					<tr><td><li>Asma Ben Abacha</td><td>NIH/NLM</td></li> <td> </td></tr>
					<tr><td><li>Thoudam Doren Singh</td><td>National Institute of Technology, Silchar, India</td></li> <td> </td></tr>
					<tr><td><li>Dhivya Chinnappa</td><td>Thomson Reuters</td></li> <td> </td></tr>
					<tr><td><li>Shailza Jolly</td><td>TU Kaiserslautern</td></li> <td> </td></tr>
					<tr><td><li>Alok Singh</td><td>National Institute of Technology, Silchar, India</td></li> <td> </td></tr>
					<tr><td><li>Mohamed Elhoseiny</td><td>KAUST</td></li> <td> </td></tr>
					<tr><td><li>Marimuthu Kalimuthu</td><td>Saarland University</td></li> <td> </td></tr>
					<tr><td><li>Simon Dobnik</td><td>University of Gothenburg</td></li> <td> </td></tr>
					<tr><td><li>Shruti Palaskar</td><td>CMU</td></li> <td> </td></tr>
					<tr><td><li>Yuankai Qi</td><td>University of Adelaide</td></li> <td> </td></tr>
        		</table>
				<br/>
<!--                    <p class="lead"  style="text-align:justify">-->
<!--						<ul class="lead">-->
<!--							<li> <b>Andrea Zunino</b>, Istituto Italiano di Tecnologia, <i>Italy</i>  </li>-->
<!--							<li> <b>Asako Kanezaki</b>, AIST, <i>Japan</i>  </li>-->
<!--							<li> <b>Christian Heipke</b>, Leibniz Universität Hannover, <i>Germany</i>  </li>-->
<!--							<li> <b>Christop Reinders</b>, Leibniz Universität Hannover, <i>Germany</i>  </li>-->
<!--							<li> <b>Devis Tuia</b>, Wageningen University and Research, <i>Netherlands</i> </li>-->
<!--							<li> <b>Elisa Ricci</b>, University of Perugia, <i>Italy</i>  </li>-->
<!--							<li> <b>Gui-Song Xia</b>, Wuhan University, <i> </i>China</li>-->
<!--							<li> <b>Hai Huang</b>, Bundeswehr University Munich, <i>Germany</i> </li>-->
<!--							<li> <b>Hanno Ackermann</b>, Leibniz University Hannover, <i>Germany</i> </li>-->
<!--							<li> <b>Jacopo Cavazza</b>, Istituto Italiano di Tecnologia, <i>Italy</i>  </li>-->
<!--							<li> <b>Markus Gerke</b>, TU Braunschweig, <i>Germany</i> </li>-->
<!--							<li> <b>Riccardo Volpi</b>, Istituto Italiano di Tecnologia, <i>Italy</i>  </li>-->
<!--							<li> <b>Valérie Gouet-Brunet</b>, LASTIG/IGN, <i>France</i> </li>-->
<!--							<li> <b>Vladimir Kniaz</b>, GosNIIAS, <i>Russia</i> </li>-->
<!--							<li> <b>Weiyao Lin</b>, Shanghai Jiao Tong university, <i>China</i> </li>-->
<!--							<li> <b>Wentong	Liao</b>, Leibniz University Hannover, <i>Germany</i> </li>-->
<!--							<li> <b>Yanpeng	Cao</b>, ZJU, <i>China</i> </li>-->
<!--							<li> <b>Yasuyuki Matsushita</b>, Osaka University, <i>Japan</i> </li>-->
<!--						</ul>-->
<!--					</p>-->


					 <!--p><a class="btn btn-embossed btn-primary" href="#" role="button">View Details</a>
					 <a class="btn btn-embossed btn-info" href="#" role="button">Visit Website</a></p-->
				</div>
            </div>
       </div>

        <!-- /.container -->


    <!-- Sponsor -->
    <!-- <div id ="sponsors" class="content-section-a" style="border-top: 0"> -->
        <!-- <div class="container">			 -->
            <!-- <div class="row">			 -->
				<!--div class="col-sm-6 pull-right wow fadeInRightBig">
                    <img class="img-responsive " src="img/ipad.png" alt="">
                </div-->				
                <!-- <div class="wow fadeInRightBig" data-animation-delay="200">    -->
                    <!-- <h3 class="section-heading"> Sponsor </h3> -->
					<!--div class="sub-title lead3">Lorem ipsum dolor sit atmet sit dolor greand fdanrh<br> sdfs sit atmet sit dolor greand fdanrh sdfs</div-->
                    <!-- <p class="lead"  style="text-align:justify"> -->
						<!-- <a href=" " target="_blank"><img src="img/sponsor/XXX.png" width="300"></a> -->
					<!-- </p> -->
					 <!--p><a class="btn btn-embossed btn-primary" href="#" role="button">View Details</a> 
					 <a class="btn btn-embossed btn-info" href="#" role="button">Visit Website</a></p-->
				<!-- </div>    -->
            <!-- </div> -->
        <!-- </div> -->
        <!-- /.container -->
    <!-- </div> -->
	
	<div id="contacts" class="content-section-c ">
			 <!-- Contacts -->
		<div class="container">
			<div class="row">
			
				<!--div class="col-sm-6 pull-right wow fadeInRightBig">
                    <img class="img-responsive " src="img/ipad.png" alt="">
                </div-->
				
                <div class="wow fadeInLeftBig" data-animation-delay="200">   
                    <h3 class="section-heading" style="color:white">Contacts</h3>
					<!--div class="sub-title lead3">Lorem ipsum dolor sit atmet sit dolor greand fdanrh<br> sdfs sit atmet sit dolor greand fdanrh sdfs</div-->
                    <p class="lead"  style="text-align:left;color:white">
						Contact the Organizing Committee: alvr-2020@googlegroups.com
					</p>
					
				</div>   
				<!-- <div class="wow fadeInLeftBig" data-animation-delay="200">   
                    <h3 class="section-heading"></h3>
                    <p class="lead"  style="text-align:right">
						©MULA2019
					</p>
					
				</div>   	 -->				
            </div>
			<div class="row">
			
						<div class="col-md-6 col-md-offset-3 text-center">
							<div >
									<div class="morph-button ">
										<button type="button"></button>
										
									</div>
							</div>
						</div>	
			</div>

		</div>
	</div>	

   
    <footer>
    
    </footer>

    <!-- JavaScript -->
    <script src="js/jquery-1.10.2.js"></script>
    <script src="js/bootstrap.js"></script>
	<script src="js/owl.carousel.js"></script>
	<script src="js/script.js"></script>
	<!-- StikyMenu -->
	<script src="js/stickUp.min.js"></script>
	<script type="text/javascript">
	  jQuery(function($) {
		$(document).ready( function() {
		  $('.navbar-default').stickUp();
		  
		});
	  });
	
	</script>
	<!-- Smoothscroll -->
	<script type="text/javascript" src="js/jquery.corner.js"></script> 
	<script src="js/wow.min.js"></script>
	<script>
	 new WOW().init();
	</script>
	<script src="js/classie.js"></script>
	<script src="js/uiMorphingButton_inflow.js"></script>
	<!-- Magnific Popup core JS file -->
	<script src="js/jquery.magnific-popup.js"></script> 
</body>

</html>
