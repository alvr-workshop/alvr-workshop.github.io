<!-- FlatFy Theme - Andrea Galanti /-->
<!doctype html>
<!--[if lt IE 7]> <html class="no-js ie6 oldie" lang="en"> <![endif]-->
<!--[if IE 7]>    <html class="no-js ie7 oldie" lang="en"> <![endif]-->
<!--[if IE 8]>    <html class="no-js ie8 oldie" lang="en"> <![endif]-->
<!--[if IE 9]>    <html class="no-js ie9" lang="en"> <![endif]-->
<!--[if gt IE 9]><!--> <html> <!--<![endif]-->
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, user-scalable=no, initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0">
    <meta name="description" content="3rd Workshop on Advances in Language and Vision Research (ALVR 2024) in conjunction with ACL 2024, August 16, 2024, Bangkok, Thailand">
    <meta name="author" content="">

    <title>ALVR 2024</title>

    <!-- Bootstrap core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom Google Web Font -->
    <link href="font-awesome/css/font-awesome.min.css" rel="stylesheet">
    <link href='http://fonts.googleapis.com/css?family=Lato:100,300,400,700,900,100italic,300italic,400italic,700italic,900italic' rel='stylesheet' type='text/css'>
    <link href='http://fonts.googleapis.com/css?family=Arvo:400,700' rel='stylesheet' type='text/css'>

    <!-- Custom CSS-->
    <link href="css/general.css" rel="stylesheet">

    <!-- Owl-Carousel -->
    <link href="css/custom.css" rel="stylesheet">
    <link href="css/owl.carousel.css" rel="stylesheet">
    <link href="css/owl.theme.css" rel="stylesheet">
    <link href="css/style.css" rel="stylesheet">
    <link href="css/animate.css" rel="stylesheet">

    <!-- Magnific Popup core CSS file -->
    <link rel="stylesheet" href="css/magnific-popup.css">

    <script src="js/modernizr-2.8.3.min.js"></script> <!-- Modernizr /-->
    <script type="application/ld+json">
        {
          "@context": "http://schema.org",
          "@type": "Event",
          "name": "3rd Workshop on Advances in Language and Vision Research (ALVR 2024)",
          "startDate": "2024-08-16",
          "endDate": "2024-08-16",
          "location": {
            "@type": "Place",
            "name": "Conference Venue",
            "address": {
              "@type": "PostalAddress",
              "streetAddress": "Street Name",
              "addressLocality": "Bangkok",
              "addressCountry": "Thailand"
            }
          },
          "url": "https://alvr-workshop.github.io/2024/",
          "description": "The 3rd Workshop on Advances in Language and Vision Research (ALVR) will be held in conjunction with ACL 2024 on August 16, 2024, in Bangkok, Thailand. The workshop will cover various topics in the intersection of language and vision research."
        }
    </script>
        
        
    <!--[if IE 9]>
    <script src="js/PIE_IE9.js"></script>
    <![endif]-->
    <!--[if lt IE 9]>
    <script src="js/PIE_IE678.js"></script>
    <![endif]-->

    <!--[if lt IE 9]>
    <script src="js/html5shiv.js"></script>
    <![endif]-->

</head>

<body id="home">

<!-- Preloader -->
<!--
<div id="preloader">
    <div id="status"></div>
</div>
-->

<!-- FullScreen -->
<div class="intro-header">
    <div class="col-xs-12 text-center">
        <div style="background-color: rgba(0,0,0,0.5)">
        <h1 class="h1_home wow " data-wow-delay="0.4s" style="color:white; text-shadow: 0 0 8px black;">3<sup>rd</sup> Workshop on Advances in Language and Vision Research (ALVR)</h1>
        <h3 class="h3_home wow " data-wow-delay="0.6s" style="color:white; text-shadow: -1px 0 black, 0 1px black, 1px 0 black, 0 -1px black, 2px 2px 4px black">
            In conjunction with <a style="color:white" href="https://2024.aclweb.org/" target="_blank"><b>ACL 2024</b></a> <br>
            August 16<sup>st</sup> 2024 (Full Day) <br>
            Location: Bangkok, Thailand,  Lotus Suite 11 (Floor 22)
        </h3>
        </div>
        <!--
        <h3 class="h3_home wow " data-wow-delay="0.6s" style="color:black; background-color:rgba(255,255,255,0.5); text-shadow: -1px 0 black, 0 1px black, 1px 0 black, 0 -1px black, 2px 2px 4px black">
            In conjunction with <a style="color:black" href="https://2024.naacl.org/" target="_blank"><b>NAACL 2024</b></a> <br>
            June 11<sup>st</sup> 2024 (Full Day) <br>
            Location: Mexico City, Mexico
        </h3>
        -->
        <!--ul class="list-inline intro-social-buttons">
            <li><a href="https://twitter.com/galantiandrea" class="btn  btn-lg mybutton_cyano wow " data-wow-delay="0.8s"><span class="network-name">Twitter</span></a>
            </li>
            <li id="download" ><a href="#downloadlink" class="btn  btn-lg mybutton_standard wow swing wow " data-wow-delay="1.2s"><span class="network-name">Free Download</span></a>
            </li>
        </ul-->
    </div>
    <!-- /.container -->
    <div class="col-xs-12 text-center abcen wow ">
        <div class="button_down ">
            <a class="imgcircle wow bounceInUp" data-wow-duration="1.5s"  href="#scope"> <img class="img_scroll" src="img/icon/circle.png" alt=""> </a>
        </div>
    </div>
    <span class="intro-caption">Photo by <a href="http://www.istockphoto.com/">boykpe</a> on <a href="http://www.unsplash.com">iStock</a> </span>
</div>

<!-- NavBar-->
<nav class="navbar-default" role="navigation">
    <div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="#home">ALVR 2024</a>
        </div>

        <div class="collapse navbar-collapse navbar-right navbar-ex1-collapse">
            <ul class="nav navbar-nav">
                <li class="menuItem"><a href="#scope">Home</a></li>
                <!-- <li class="menuItem"><a href="#dates">Dates</a></li> -->
                <!-- <li class="menuItem"><a href="#submit">Submission</a></li> -->
                <!-- <li class="menuItem"><a href="#proceedings">Proceedings</a></li> -->
                <li class="menuItem"><a href="#program">Schedule</a></li>
                <li class="menuItem"><a href="#accepted-papers">Accepted Papers</a></li>
                <li class="menuItem"><a href="#invited">Invited Speakers</a></li>
                <li class="menuItem"><a href="#organizers">Organizers</a></li>
                <!-- <li class="menuItem"><a href="#sponsors">Sponsor</a></li>					 -->
                <!-- <li class="menuItem"><a href="#contacts">Contact</a></li> -->
                <li class="menuItem"><a href="2020/index.html">ALVR 2020</a></li>
                <li class="menuItem"><a href="2021/index.html">ALVR 2021</a></li>
            </ul>
        </div>

    </div>
</nav>

<!-- Scope -->
<div id ="scope" class="content-section-a" style="border-top: 0">

    <div class="container">

        <div class="row">

            <!--div class="col-sm-6 pull-right wow ">
                <img class="img-responsive " src="img/ipad.png" alt="">
            </div-->
            <!--<img  class="img-responsive"  style="width: 20vw"  src="img/cvpr_logo.png" alt="">
            <img  class="img-responsive"  style="width: 20vw"  src="img/cvf.jpg" alt="">
            <img  class="img-responsive"  style="width: 10vw"  src="img/organizer/vgis.png" alt="">  -->
            <div class="wow " data-animation-delay="200">
                <!--<h3 class="section-heading">ACL 2020 Workshop on Advances in Language and Vision Research</h3>-->
                <h3 class="section-heading">3<sup>rd</sup> Workshop on Advances in Language and Vision Research</h3>

                <!--div class="sub-title lead3">Lorem ipsum dolor sit atmet sit dolor greand fdanrh<br> sdfs sit atmet sit dolor greand fdanrh sdfs</div-->
                <p>
                    Language and vision research has attracted great attention from both natural language processing (NLP) and computer vision (CV) researchers. Gradually, this area is shifting from passive perception, templated language, and synthetic imagery/environments to active perception, natural language, and photo-realistic simulation or real world deployment. 
                    <!--
                    Thus far, few workshops on language and vision research have been organized by groups from the NLP community. 
                    We are organizing <b>the second workshop on Advances in Language and Vision Research (ALVR)</b> in order to promote the frontier of language and vision research and to bring interested researchers together to discuss how to best tackle and solve real-world problems in this area.<br/><br/>
                    -->
                    This workshop covers (but is not limited to) the following topics: <br/>
                <ul>
                    <li> Self-supervised vision and language pre-training; </li>
                    <li> New tasks and datasets that provide real-world solutions in language and vision; </li>
                    <li> Text-to-image/video generation and text-guided image/video editing; </li>
                    <li> External knowledge integration in visual and language understanding;</li>
                    <li> Visually-grounded natural language understanding and generation;</li>
                    <li> Language-grounded visual recognition and reasoning;</li>
                    <li> Language-grounded embodied agents, e.g., vision-and-language navigation;</li> 
                    <li> Visually-grounded multilingual study, e.g., multimodal machine translation; </li>
                    <li> Shortcomings of the existing large vision\&language models on downstream tasks and solutions;</li> 
                    <li> Ethics and bias on large vision\&language model.</li>
                    <li> Multidisciplinary study that may involve linguistics, cognitive science, robotics, etc.</li>
                    <li> Explainability and interpretability on large vision\&language model.</li>
                </ul>
                </p>

                <!-- <a id="important-dates" class="anchor" href="#accepted-papers" aria-hidden="true"><span class="octicon octicon-link"></span></a>_..

                 <!--p><a class="btn btn-embossed btn-primary" href="#" role="button">View Details</a>
                 <a class="btn btn-embossed btn-info" href="#" role="button">Visit Website</a></p-->
            </div>
        </div>
    </div>
    <!-- /.container -->
</div>

<!--
<div id ="dates" class="content-section-b" style="border-top: 0">
    <div class="container">
        <div class="row">
            <div class="wow " data-animation-delay="200">
                <h3 class="section-heading">Important Dates</h3>
                <ul>
                    <li>Archival track:</li>
                    <ul>
                      <li><s>Paper Submission Due Date: March 22, 2024</s></li>
                      <li><s>Notification of acceptance: April 15, 2024</s></li>
                        <li><s>Camera-ready papers due: April 26, 2024</s></li>
                    </ul>
                    <li>Non-archival track:</li>
                    <ul>
                        <li><s>Paper Submission Due Date: April 30, 2024</s></li>
                        <li><s>Notification of acceptance: May 14, 2024</s></li>
                        <li><s>Camera-ready papers due: May 24, 2024</s></li>
                    </ul>
                    <li>Workshop Date: June 11, 2024</li>
                </ul>
            </div>
        </div>
    </div>
</div> 
-->
<!-- /dates -->

<!-- Call for Papers -->
<div id="submit" class="content-section-b">
    <div class="container">
        <div class="row">
            <div class="wow" data-animation-delay="200">
                <h3 class="section-heading">Call for Papers</h3>

                <p>Long papers may consist of up to 8 pages of content, plus unlimited pages for references and an appendix; final versions of long papers will be given one additional page of content (up to 9 pages) so that reviewers’ comments can be considered.</p>

                <p>Short papers may consist of up to 4 pages of content, plus unlimited references and an appendix. Short papers will be given 5 content pages in the proceedings upon acceptance. Authors are encouraged to use this additional page to address reviewers’ comments in their final versions.</p>

                <p>We are also including a non-archival track to allow dual submission of work to ALVR 2024 and other conferences/journals. Space permitting, these submissions will still participate and present their work in the workshop and will be hosted on the workshop website but will not be included in the official proceedings. Please apply the ACL format and submit through <a href="https://openreview.net/group?id=aclweb.org/ACL/2024/Workshop/ALVR" style="color:orange">openreview</a> but indicate that this is a cross-submission (non-archival) at the bottom of the submission form.</p>

                <p>The submission website is <a href="https://openreview.net/group?id=aclweb.org/ACL/2024/Workshop/ALVR" style="color:orange">https://openreview.net/group?id=aclweb.org/ACL/2024/Workshop/ALVR</a>.</p>
            </div>
        </div>
    </div>
    <!-- /.container -->
</div>
<!-- End Call for Papers -->

<!--
<div id ="proceedings" class="content-section-b">

    <div class="container">

        <div class="row">

            <div class="wow " data-animation-delay="200">
                <h3 class="section-heading">Proceedings</h3>
                <li>TBD</li>
            </div>
        </div>
    </div>
</div>
-->


<!-- Program -->
<div id ="program" class="content-section-b">

    <div class="container">
        <div class="row">
            <div class="wow " data-animation-delay="200">
                <h3 class="section-heading">Schedule (ICT, UTC+7)</h3>
                <!--<p class="lead">
                    <b>ATTENTION! The video recording of the whole workshop is located at <a href="https://slideslive.com/38931798/w9-alvr-live-stream" style="color:orange"> <u>https://slideslive.com/38931798/w9-alvr-live-stream</u></a></b>. Some pre-recorded talks are listed below, while others are included in the workshop recording.
                </p>-->
            </div>

            <table class="table" style="width:100%; table-layout:fixed">
                <tr>
                    <th scope="col" width="15%">Time (ICT) </th>
                    <th scope="col" width="60%">Event </th>
                    <th scope="col" width="20%">Who </th>
                    </th>

                <tr class="info">
                    <td>8:10-8:20</td>
                    <td><strong>Opening Remarks</strong></td>
                    <td>Workshop Organizers</td>
                </tr>

                <tr class="success">
                    <td>8:20-9:10</td>
                    <td>Invited Talk 1: <strong>Foundations of Multimodal Interactions and Multisensory Foundation Models</strong>
                      <a class="talk-toggle" data-toggle="collapse" data-target="#schedule-talk1">Abstract</a>
                      <div class="collapse" id="schedule-talk1">Building multisensory AI systems that learn from multiple sensory inputs such as text, speech, video, real-world sensors, wearable devices, and medical data holds great promise for impact in many scientific areas with practical benefits, such as in supporting human health and well-being, enabling multimedia content processing, and enhancing real-world autonomous agents. In this talk, I will discuss my research on the machine learning principles of multisensory intelligence, as well as practical methods for building multisensory foundation models over many modalities and tasks. In the first half, I will present a theoretical framework formalizing how modalities interact with each other to give rise to new information for a task. These interactions are the basic building blocks in all multimodal problems, and their quantification enables users to understand their multimodal datasets and design principled approaches to learn these interactions. In the second part, I will discuss our collaborative efforts in scaling AI to many modalities and tasks for real-world impact on affective computing, mental health, and cancer prognosis.</div>
                    </td>
                    <td>Paul Liang</td>
                </tr>

                <tr class="success">
                    <td>9:10-10:00</td>
                    <td>Invited Talk 2: <strong>Connecting 3D and Language</strong>
                      <a class="talk-toggle" data-toggle="collapse" data-target="#schedule-talk2">Abstract</a>
                      <div class="collapse" id="schedule-talk2">People communicate about objects, scenes, and spatial relations in the real world using natural language. In this talk, I will give an overview of recent work that explores how to endow computational systems with the ability to connect natural language and 3D representations. Concretely, I will summarize projects that develop neural models for localizing and describing objects in 3D scenes using natural language, generating 3D content from text, and other recent work exploring tasks that require connections between natural language and 3D representations.</div>
                    </td>
                    <td>Angel Chang</td>
                </tr>
                
                <tr>
                    <td>10:00-10:30</td>
                    <td><strong>Coffee Break</strong><br /></td>
                    <td></td>
                </tr>

                <tr class="success">
                    <td>10:30-11:20</td>
                    <td>Invited Talk 3: <strong>Planning and Reasoning for Multi-Agent Embodied Tasks: Benchmarking and Evaluation</strong>
                      <a class="talk-toggle" data-toggle="collapse" data-target="#schedule-talk3">Abstract</a>
                      <div class="collapse" id="schedule-talk3">Language is never used in isolation. It is articulated, understood, and contextualized against the speaker’s history, actions, and environment. To this end, we have developed a series of benchmarks to evaluate language comprehension in dynamic, embodied settings where robots execute instructions given by humans. First, I will discuss Situated Instruction Following, where the meaning of instructions is revealed through the past actions and anticipated future behaviors of the human speaker. Then, I will introduce PARTNR, a multi-agent benchmark that involves a human and their robotic assistant collaboratively completing tasks under various constraints, including spatial, temporal, and heterogeneity challenges. Lastly, I will present GOAT-Bench, a robot navigation benchmark in a continual learning framework where the navigation target is defined through multiple modalities, including images and natural language descriptions of objects. I will share our analysis of these benchmarks, demonstrating how state-of-the-art LLMs and large-scale multi-modal models struggle with tracking task progression, managing ambiguities, and dividing tasks effectively.</div>
                    </td>
                    <td>Roozbeh Mottaghi</td>
                </tr>
                
                <tr class="success">
                    <td>11:20-12:10</td>
                    <td>Invited Talk 4: <strong>The Role of Joint Embodiment in Situated Language-Based Interactions</strong>
                      <a class="talk-toggle" data-toggle="collapse" data-target="#schedule-talk4">Abstract</a>
                      <div class="collapse" id="schedule-talk4">Large-scale pretraining has become the standard solution to automated reasoning over text and/or visual perception. But how far does this approach get us to systems that generalize to language use in realistic multi-agent situated interactions? First, I will talk about existing work in evaluating the spatial and compositional reasoning capabilities of current multimodal LMs. Then, I will talk about how these benchmarks miss a key aspect of real-world situated interactions: joint embodiment. I will discuss how joint embodiment in a shared world supports perspective-taking, an underlooked aspect of situated reasoning, and introduce a new environment and benchmark for studying the influence of perspective-taking on language use in interaction.</div>
                    </td>
                    <td>Alane Suhr</td>
                </tr>

                <tr>
                    <td>12:10-13:30</td>
                    <td><strong>Lunch</strong><br /></td>
                    <td></td>
                </tr>

                <tr class="success">
                    <td>13:30-14:20</td>
                    <td>Invited Talk 5: <strong>Toward Vision and Richer Language(s)</strong>
                      <a class="talk-toggle" data-toggle="collapse" data-target="#schedule-talk5">Abstract</a>
                      <div class="collapse" id="schedule-talk5">How rich is the language in vision and language research? Arguably for some time, visual understanding has been the first class citizen of this research direction. In this talk, I will present research projects in the past several years that aim at elevating the role of language(s). This spans improving specificity and informativeness, reasoning about the language in pixels, as well as going beyond declarative, literal, and English languages. Collectively, this moves us toward tightly connecting vision with rich(er) languages.</div>
                    </td>
                    <td>Soravit "Beer" Changpinyo</td>
                </tr>

                <tr class="info">
                    <td>14:20-15:00</td>
                    <td><strong>Poster Highlight</strong><br /></td>
                    <td></td>
                </tr>
            
                <tr>
                    <td>15:00-15:50</td>
                    <td><strong>Poster Session, Coffee Break</strong><br /></td>
                    <td></td>
                </tr>

                <tr class="success">
                    <td>15:50-16:40</td>
                    <td>Invited Talk 6: <strong>Representing Illustrative Visual Semantics with Descriptive Language</strong>
                      <a class="talk-toggle" data-toggle="collapse" data-target="#schedule-talk6">Abstract</a>
                      <div class="collapse" id="schedule-talk6">Contemporary visual semantic representations predominantly revolve around commonplace objects found in everyday images and videos, ranging from ladybugs and bunnies to airplanes. However, crucial visual cues extend beyond mere object recognition and interaction. They encompass a spectrum of richer semantics, including vector graphics (e.g., angles, mazes), fine-grained attributes and affordances. Moreover, they entail intricate visual dynamics, such as object interactions, actions, and activities. Regrettably, traditional visual representations relying solely on pixels and regions fail to fully encapsulate these nuances. In this task, I propose to design intermediate symbolic semantic representations to precisely describe and aggregate these low-level visual signals. This augmentation promises to enhance their utility as inputs for large language models or vision-language models, thereby facilitating high-level knowledge reasoning and discovery tasks. I will present several applications range from playful maze solving and fine-grained concept recognition to video activity detection.</div>
                    </td>
                    <td>Heng Ji</td>
                </tr>

                <tr class="info">
                    <td>16:40-17:20</td>
                    <td><strong>Panel Session</strong><br /></td>
                    <td>Xin Eric Wang, Paul Liang, Roozbeh Mottaghi, Soravit "Beer" Changpinyo, Heng Ji</td>
                </tr>

            </table>

            <div class="container">
                <div class="wow " data-animation-delay="200">
                </div>
            </div>
        </div>
    </div>
</div>


<div id ="accepted-papers" class="content-section-b">
    <div class="container">
        <div class="row">
            <div class="wow " data-animation-delay="200">
                <h3 class="section-heading">Accepted Papers</h3>
                <h4 class="section-heading">Archival track:</h4>
                <ul>
                  <li>
                    <b> WISMIR3: A Multi-Modal Dataset to Challenge Text-Image Retrieval Approaches </b> 
                    <!-- (<a href="./proceedings/2.pdf" style="color:orange">PDF</a> | <a href="https://youtu.be/Y84afI9A1PI" style="color:orange">Video</a>) -->
                    <br>
                    Florian Schneider, Chris Biemann 
                  </li>
                  <li>
                    <b> mBLIP: Efficient Bootstrapping of Multilingual Vision-LLMs </b> 
                    <!-- (<a href="./proceedings/5.pdf" style="color:orange">PDF</a> | <a href="https://youtu.be/E7KtUbQDZPI" style="color:orange">Video</a>) -->
                    <br>
                    Gregor Geigle, Abhay Jain, Radu Timofte, Goran Glavaš 
                  </li>
                  <li>
                    <b> LMPT: Prompt Tuning with Class-Specific Embedding Loss for Long-Tailed Multi-Label Visual Recognition </b> 
                    <!-- (<a href="./proceedings/3.pdf" style="color:orange">PDF</a> | <a href="https://youtu.be/Link1" style="color:orange">Video</a>) -->
                    <br>
                    Peng Xia, Di Xu, Ming Hu, Lie Ju, Zongyuan Ge
                  </li>
                  <li>
                    <b> Negative Object Presence Evaluation (NOPE) to Measure Object Hallucination in Vision-Language Models </b> 
                    <!-- (<a href="./proceedings/4.pdf" style="color:orange">PDF</a> | <a href="https://youtu.be/Link2" style="color:orange">Video</a>) -->
                    <br>
                    Holy Lovenia, Wenliang Dai, Samuel Cahyawijaya, Ziwei Ji, Pascale Fung
                  </li>
                  <li>
                    <b> How and where does CLIP process negation? </b> 
                    <!-- (<a href="./proceedings/6.pdf" style="color:orange">PDF</a> | <a href="https://youtu.be/Link3" style="color:orange">Video</a>) -->
                    <br>
                    Vincent Quantmeyer, Pablo Mosteiro, Albert Gatt
                  </li>
                  <li>
                    <b> Enhancing Continual Learning in Visual Question Answering with Modality-Aware Feature Distillation </b> 
                    <!-- (<a href="./proceedings/7.pdf" style="color:orange">PDF</a> | <a href="https://youtu.be/Link4" style="color:orange">Video</a>) -->
                    <br>
                    Malvina Nikandrou, Georgios Pantazopoulos, Ioannis Konstas, Alessandro Suglia
                  </li>
                  <li>
                    <b> English-to-Japanese Multimodal Machine Translation Based on Image-Text Matching of Lecture Videos </b> 
                    <!-- (<a href="./proceedings/8.pdf" style="color:orange">PDF</a> | <a href="https://youtu.be/Link5" style="color:orange">Video</a>) -->
                    <br>
                    Ayu Teramen, Takumi Ohtsuka, Risa Kondo, Tomoyuki Kajiwara, Takashi Ninomiya
                  </li>
                  <li>
                    <b> VideoCoT: A Video Chain-of-Thought Dataset with Active Annotation Tool </b> 
                    <!-- (<a href="./proceedings/9.pdf" style="color:orange">PDF</a> | <a href="https://youtu.be/Link6" style="color:orange">Video</a>) -->
                    <br>
                    Yan Wang, Yawen Zeng, Jingsheng Zheng, Xiaofen Xing, Jin Xu, Xiangmin Xu
                  </li>
                  <li>
                    <b> Enhancing Conceptual Understanding in Multimodal Contrastive Learning through Hard Negative Samples </b> 
                    <!-- (<a href="./proceedings/10.pdf" style="color:orange">PDF</a> | <a href="https://youtu.be/Link7" style="color:orange">Video</a>) -->
                    <br>
                    Philipp J. Rösch, Norbert Oswald, Michaela Geierhos, Jindřich Libovický
                  </li>
                  <li>
                    <b> Vision Language Models for Spreadsheet Understanding: Challenges and Opportunities </b> 
                    <!-- (<a href="./proceedings/11.pdf" style="color:orange">PDF</a> | <a href="https://youtu.be/Link8" style="color:orange">Video</a>) -->
                    <br>
                    Shiyu Xia, Junyu Xiong, Haoyu Dong, Jianbo Zhao, Yuzhang Tian, Mengyu Zhou, Yeye He, Shi Han, Dongmei Zhang
                  </li>
                  <li>
                    <b> SlideAVSR: A Dataset of Paper Explanation Videos for Audio-Visual Speech Recognition </b> 
                    <!-- (<a href="./proceedings/12.pdf" style="color:orange">PDF</a> | <a href="https://youtu.be/Link9" style="color:orange">Video</a>) -->
                    <br>
                    Hao Wang, Shuhei Kurita, Shuichiro Shimizu, Daisuke Kawahara
                  </li>
                  <li>
                    <b> Causal and Temporal Inference in Visual Question Generation by Utilizing Pre-trained Models </b> 
                    <!-- (<a href="./proceedings/13.pdf" style="color:orange">PDF</a> | <a href="https://youtu.be/Link10" style="color:orange">Video</a>) -->
                    <br>
                    Zhanghao Hu, Frank Keller
                  </li>
                  <li>
                    <b> Improving Vision-Language Cross-Lingual Transfer with Scheduled Unfreezing </b> 
                    <!-- (<a href="./proceedings/14.pdf" style="color:orange">PDF</a> | <a href="https://youtu.be/Link11" style="color:orange">Video</a>) -->
                    <br>
                    Max Reinhardt, Gregor Geigle, Radu Timofte, Goran Glavaš
                  </li>
                  <li>
                    <b> Automatic Layout Planning for Visually-Rich Documents with Instruction-Following Models </b> 
                    <!-- (<a href="./proceedings/15.pdf" style="color:orange">PDF</a> | <a href="https://youtu.be/Link12" style="color:orange">Video</a>) -->
                    <br>
                    Wanrong Zhu, Ruiyi Zhang, Jennifer Healey, William Yang Wang, Tong Sun
                  </li>
                  <li>
                    <b> SEA-VQA: Southeast Asian Cultural Context Dataset For Visual Question Answering </b> 
                    <!-- (<a href="./proceedings/16.pdf" style="color:orange">PDF</a> | <a href="https://youtu.be/Link13" style="color:orange">Video</a>) -->
                    <br>
                    Norawit Urailertprasert, Peerat Limkonchotiwat, Supasorn Suwajanakorn, Sarana Nutanong
                  </li>
                  <li>
                    <b> Wiki-VEL: Visual Entity Linking for Structured Data on Wikimedia Commons </b> 
                    <!-- (<a href="./proceedings/17.pdf" style="color:orange">PDF</a> | <a href="https://youtu.be/Link14" style="color:orange">Video</a>) -->
                    <br>
                    Philipp Bielefeld, Jasmin Geppert, Necdet Güven, Melna Treesa John, Adrian Ziupka, Lucie-Aimée Kaffee, Russa Biswas, Gerard de Melo
                  </li>
                  <li>
                    <b> VerbCLIP: Improving Verb Understanding in Vision-Language Models with Compositional Structures </b> 
                    <!-- (<a href="./proceedings/18.pdf" style="color:orange">PDF</a> | <a href="https://youtu.be/Link15" style="color:orange">Video</a>) -->
                    <br>
                    Hadi Wazni, Kin Ian Lo, Mehrnoosh Sadrzadeh
                  </li>
                  <li>
                    <b> Evolutionary Reward Design and Optimization with Multimodal Large Language Models </b> 
                    <!-- (<a href="./proceedings/18.pdf" style="color:orange">PDF</a> | <a href="https://youtu.be/Link15" style="color:orange">Video</a>) -->
                    <br>
                    Ali Emre Narin
                  </li>           
                </ul>

                <h4 class="section-heading">Non-archival track:</h4>
               
                <ul>
                    <li>
                        <b> FaithScore: Fine-grained Evaluations of Hallucinations in Large Vision-Language Models </b> 
                        <!-- (<a href="./proceedings/19.pdf" style="color:orange">PDF</a> | <a href="https://youtu.be/Link1" style="color:orange">Video</a>) -->
                        <br>
                        Liqiang Jing, Ruosen Li, Yunmo Chen, Mengzhao Jia, Xinya Du
                      </li>
                      <li>
                        <b> HGCLIP: Exploring Vision-Language Models with Graph Representations for Hierarchical Understanding </b> 
                        <!-- (<a href="./proceedings/20.pdf" style="color:orange">PDF</a> | <a href="https://youtu.be/Link2" style="color:orange">Video</a>) -->
                        <br>
                        Peng Xia, Xingtong Yu, Ming Hu, Lie Ju, Zhiyong Wang, Peibo Duan, Zongyuan Ge
                      </li>
                      <li>
                        <b> Prometheus-Vision: Vision-Language Model as a Judge for Fine-Grained Evaluation </b> 
                        <!-- (<a href="./proceedings/21.pdf" style="color:orange">PDF</a> | <a href="https://youtu.be/Link3" style="color:orange">Video</a>) -->
                        <br>
                        Seongyun Lee, Seungone Kim, Sue Hyun Park, Geewook Kim, Minjoon Seo
                      </li>
                      <li>
                        <b> Can Large Vision Language Models Understand and Reason with Charts? An Empirical Study into the Capabilities and Limitations of LVLMs </b> 
                        <!-- (<a href="./proceedings/22.pdf" style="color:orange">PDF</a> | <a href="https://youtu.be/Link4" style="color:orange">Video</a>) -->
                        <br>
                        Mohammed Saidul Islam, Raian Rahman, Ahmed Masry, Md Tahmid Rahman Laskar, Mir Tafseer Nayeem, Enamul Hoque
                      </li>
                      <li>
                        <b> MM-SOC: Benchmarking Multimodal Large Language Models in Social Media Platforms </b> 
                        <!-- (<a href="./proceedings/23.pdf" style="color:orange">PDF</a> | <a href="https://youtu.be/Link5" style="color:orange">Video</a>) -->
                        <br>
                        Yiqiao Jin, Minje Choi, Gaurav Verma, Jindong Wang, Srijan Kumar
                      </li>
                      <li>
                        <b> ViCor: Bridging Visual Understanding and Commonsense Reasoning with Large Language Models </b> 
                        <!-- (<a href="./proceedings/24.pdf" style="color:orange">PDF</a> | <a href="https://youtu.be/Link6" style="color:orange">Video</a>) -->
                        <br>
                        Kaiwen Zhou, Kwonjoon Lee, Teruhisa Misu, Xin Eric Wang
                      </li>
                      <li>
                        <b> HateSieve: A Contrastive Learning Framework for Detecting and Segmenting Hateful Content in Multimodal Memes </b> 
                        <!-- (<a href="./proceedings/25.pdf" style="color:orange">PDF</a> | <a href="https://youtu.be/Link7" style="color:orange">Video</a>) -->
                        <br>
                        Xuanyu Su, Yansong Li, Diana Inkpen, Nathalie Japkowicz
                      </li>
                      <li>
                        <b> ANNA: Abstractive Text-to-Image Synthesis with Filtered News Captions </b> 
                        <!-- (<a href="./proceedings/26.pdf" style="color:orange">PDF</a> | <a href="https://youtu.be/Link8" style="color:orange">Video</a>) -->
                        <br>
                        Aashish Anantha Ramakrishnan, Sharon X Huang, Dongwon Lee
                      </li>
                      <li>
                        <b> Multimodal Reranking for Knowledge-Intensive Visual Question Answering </b> 
                        <!-- (<a href="./proceedings/27.pdf" style="color:orange">PDF</a> | <a href="https://youtu.be/Link9" style="color:orange">Video</a>) -->
                        <br>
                        Haoyang Wen, Honglei Zhuang, Hamed Zamani, Alexander G Hauptmann, Michael Bendersky
                      </li>
                      <li>
                        <b> Mitigating Hallucinations in Large Vision-Language Models (LVLMs) via Language-Contrastive Decoding (LCD) </b> 
                        <!-- (<a href="./proceedings/28.pdf" style="color:orange">PDF</a> | <a href="https://youtu.be/Link10" style="color:orange">Video</a>) -->
                        <br>
                        Avshalom Manevich, Reut Tsarfaty
                      </li>
                      <li>
                        <b> Natural Language Can Facilitate Sim2Real Transfer </b> 
                        <!-- (<a href="./proceedings/29.pdf" style="color:orange">PDF</a> | <a href="https://youtu.be/Link11" style="color:orange">Video</a>) -->
                        <br>
                        Albert Yu, Adeline Foote, Ray Mooney, Roberto Martín-Martín
                      </li>
                      <li>
                        <b> Multi-Object Hallucination in Vision-Language Models </b> 
                        <!-- (<a href="./proceedings/30.pdf" style="color:orange">PDF</a> | <a href="https://youtu.be/Link12" style="color:orange">Video</a>) -->
                        <br>
                        Xuweiyi Chen, Ziqiao Ma, Xuejun Zhang, Sihan Xu, Shengyi Qian, Jianing Yang, David Fouhey, Joyce Chai
                      </li>                                        
                </ul>
            </div>
        </div>
    </div>
</div>
    

<!-- Invited Speakers -->
<div id ="invited" class="content-section-b">

    <div class="container">

        <div class="row">
            <div class="container">

                <div class="row wow "  data-animation-delay="200">
                    <h3 class="section-heading">Invited Speakers</h3>
                    <!--<p>(presentation order)</p>-->
                    <br>
                </div>

                <div class="row wow "  data-animation-delay="200">
                    <div class="col-sm-3 wow"  data-animation-delay="200">
                        <center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/invited/paul.jpg" alt=""></center>
                        <h4 class="section-heading"><center><a href="https://pliang279.github.io/" target="blank">Paul Liang</a></center></h4>
                    </div>
                    <div class="col-sm-8 col-md-offset-1 wow" style="text-align:justify" data-animation-delay="200">
                        <p>Dr. Paul Liang is an Assistant Professor at MIT Media Lab and EECS, whose research focuses on advancing the foundations of multisensory artificial intelligence to enhance human experiences across various domains. Recognized with prestigious awards including the Siebel Scholars Award, Waibel Presidential Fellowship, Facebook PhD Fellowship, and multiple best paper awards, Liang's work explores the integration of diverse sensory channels such as text, speech, audio, video, and physiological signals to create AI systems that interact seamlessly with the world. His research interests span from enhancing human physical, emotional, and social well-being through AI, to augmenting human creativity with multimedia generative AI, and addressing critical issues in real-world human-AI interaction such as fairness, trust, and privacy. Beyond research, he has also been honored with the Alan J. Perlis Graduate Student Teaching Award for his contributions to multimodal machine learning education.</p>
                    </div>
                </div>
                <p></p>

                <div class="row wow "  data-animation-delay="200">
                    <div class="col-sm-3 wow"  data-animation-delay="200">
                        <center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/invited/angel.png" alt=""></center>
                        <h4 class="section-heading"><center><a href="https://angelxuanchang.github.io/" target="blank">Angel Chang</a></center></h4>
                    </div>
                    <div class="col-sm-8 col-md-offset-1 wow" style="text-align:justify" data-animation-delay="200">
                        <p>Dr. Angel Chang is an Assistant Professor at Simon Fraser University. Prior to this, she was a visiting research scientist at Facebook AI Research and a research scientist at Eloquent Labs working on dialogue. She received my Ph.D. in Computer Science from Stanford, where she was part of the Natural Language Processing Group and advised by Chris Manning. Her research focuses on connecting language to 3D representations of shapes and scenes and grounding of language for embodied agents in indoor environments. She has worked on methods for synthesizing 3D scenes and shapes from natural language, and various datasets for 3D scene understanding. In general, she is interested in the semantics of shapes and scenes, the representation and acquisition of common sense knowledge, and reasoning using probabilistic models. </p>
                    </div>
                </div>
                <p></p>

                <div class="row wow "  data-animation-delay="200">
                    <div class="col-sm-3 wow"  data-animation-delay="200">
                        <center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/invited/roozbeh.jpg" alt=""></center>
                        <h4 class="section-heading"><center><a href="https://roozbehm.info/" target="blank">Roozbeh Mottaghi</a></center></h4>
                    </div>
                    <div class="col-sm-8 col-md-offset-1 wow" style="text-align:justify" data-animation-delay="200">
                        <p>Dr. Roozbeh Mottaghi is a Senior Research Scientist Manager at FAIR and an Affiliate Associate Professor in Paul G. Allen School of Computer Science and Engineering at the University of Washington. Prior to joining FAIR, he was the Research Manager of the Perceptual Reasoning and Interaction Research (PRIOR) group at the Allen Institute for AI (AI2). He obtained his PhD in Computer Science in 2013 from the University of California, Los Angeles. After PhD, he joined the Computer Science Department at Stanford University as a post-doctoral researcher. His research mainly focuses on embodied AI, reasoning via perception, and learning via interaction, and his work on large-scale Embodied AI received the Outstanding Paper Award at NeurIPS 2022. </p>
                    </div>
                </div>
                <p></p>
                
                <div class="row wow "  data-animation-delay="200">
                    <div class="col-sm-3 wow"  data-animation-delay="200">
                        <center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/invited/alane.jpg" alt=""></center>
                        <h4 class="section-heading"><center><a href="https://www.alanesuhr.com/" target="blank">Alane Suhr</a></center></h4>
                    </div>
                    <div class="col-sm-8 col-md-offset-1 wow" style="text-align:justify" data-animation-delay="200">
                        <p>Dr. Alane Suhr is an Assistant Professor at UC Berkeley EECS. She received PhD in Computer Science at Cornell University, based at Cornell Tech in New York, NY, and advised by Yoav Artzi. Afterwards, she spent about a year in Seattle, WA at AI2 as a Young Investigator on the Mosaic team (led by Yejin Choi). Her research spans natural language processing, machine learning, and computer vision. She builds systems that use language to interact with people, e.g., in collaborative interactions (like CerealBar). She also designs models and datasets that address and represent problems in language grounding (e.g., NLVR), and develops learning algorithms for systems that learn language through interaction.</p>
                    </div>
                </div>
                <p></p>

                <div class="row wow "  data-animation-delay="200">
                    <div class="col-sm-3 wow"  data-animation-delay="200">
                        <center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/invited/beer.jpg" alt=""></center>
                        <h4 class="section-heading"><center><a href="https://schangpi.github.io/" target="blank">Soravit "Beer" Changpinyo</a></center></h4>
                    </div>
                    <div class="col-sm-8 col-md-offset-1 wow" style="text-align:justify" data-animation-delay="200">
                        <p>Dr. Soravit "Beer" Changpinyo is a researcher at Google DeepMind, specializing in computer vision and natural language processing, with a broad interest in machine learning and artificial intelligence. His work has earned him numerous accolades, including the prestigious Annenberg Graduate Fellowship from the University of Southern California and multiple Outstanding Reviewer Awards from top-tier conferences like CVPR and NeurIPS, where he ranked among the top reviewers. His contributions to the field are further underscored by his impactful research on zero-shot learning, transfer learning, multi-task learning, and neural network optimization, which have applications in both academic and industrial settings. Soravit's recent work includes the Gemini series. His innovative work continues to drive advancements in AI, making him a key figure in the research community.</p>
                    </div>
                </div>
                <p></p>

                <div class="row wow "  data-animation-delay="200">
                    <div class="col-sm-3 wow"  data-animation-delay="200">
                        <center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/invited/heng.png" alt=""></center>
                        <h4 class="section-heading"><center><a href="https://blender.cs.illinois.edu/hengji.html" target="blank">Heng Ji </a></center></h4>
                    </div>
                    <div class="col-sm-8 col-md-offset-1 wow" style="text-align:justify" data-animation-delay="200">
                        <p>Heng Ji is a professor at Computer Science Department, and an affiliated faculty member at Electrical and Computer Engineering Department and Coordinated Science Laboratory of University of Illinois Urbana-Champaign. She is an Amazon Scholar. She is the Founding Director of Amazon-Illinois Center on AI for Interactive Conversational Experiences (AICE). She received her B.A. and M. A. in Computational Linguistics from Tsinghua University, and her M.S. and Ph.D. in Computer Science from New York University. Her research interests focus on Natural Language Processing, especially on Multimedia Multilingual Information Extraction, Knowledge-enhanced Large Language Models and Vision-Language Models. She was selected as a "Young Scientist" by the World Laureates Association in 2023 and 2024. She was selected as "Young Scientist" and a member of the Global Future Council on the Future of Computing by the World Economic Forum in 2016 and 2017. She was named as part of Women Leaders of Conversational AI (Class of 2023) by Project Voice. The other awards she received include two Outstanding Paper Awards at NAACL2024, "AI's 10 to Watch" Award by IEEE Intelligent Systems in 2013, NSF CAREER award in 2009, PACLIC2012 Best paper runner-up, "Best of ICDM2013" paper award, "Best of SDM2013" paper award, ACL2018 Best Demo paper nomination, ACL2020 Best Demo Paper Award, NAACL2021 Best Demo Paper Award, Google Research Award in 2009 and 2014, IBM Watson Faculty Award in 2012 and 2014 and Bosch Research Award in 2014-2018. She was invited to testify to the U.S. House Cybersecurity, Data Analytics, & IT Committee as an AI expert in 2023. She was selected to participate in DARPA AI Forward in 2023. She was invited by the Secretary of the U.S. Air Force and AFRL to join Air Force Data Analytics Expert Panel to inform the Air Force Strategy 2030, and invited to speak at the Federal Information Integrity R&D Interagency Working Group (IIRD IWG) briefing in 2023. She is the lead of many multi-institution projects and tasks, including the U.S. ARL projects on information fusion and knowledge networks construction, DARPA ECOLE MIRACLE team, DARPA KAIROS RESIN team and DARPA DEFT Tinker Bell team. She has coordinated the NIST TAC Knowledge Base Population task 2010-2020. She is the Chief Editor of Data Intelligence Journal. She served as the associate editor for IEEE/ACM Transaction on Audio, Speech, and Language Processing, and the Program Committee Co-Chair of many conferences including NAACL-HLT2018 and AACL-IJCNLP2022. She was elected as the North American Chapter of the Association for Computational Linguistics (NAACL) secretary 2020-2023. Her research has been widely supported by the U.S. government agencies (DARPA, NSF, DoE, ARL, IARPA, AFRL, DHS) and industry (Amazon, Google, Bosch, IBM, Disney).</p>
                    </div>
                </div>
                <p></p>

                <!-- <div class="row wow "  data-animation-delay="200">
                    <div class="col-sm-3 wow"  data-animation-delay="200">
                        <center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/organizer/xin.jpg" alt=""></center>
                        <h4 class="section-heading"><center><a href="https://eric-xw.github.io/" target="blank">Xin Eric Wang</a></center></h4>
                    </div>
                    <div class="col-sm-8 col-md-offset-1 wow" style="text-align:justify" data-animation-delay="200">
                        <p>Xin (Eric) Wang is an Assistant Professor of Computer Science and Engineering at UC Santa Cruz. His research interests include Natural Language Processing, Computer Vision, and Machine Learning, with an emphasis on Multimodal, Generative, and Embodied AI. He worked at Google AI, Facebook AI Research, Microsoft Research, and Adobe Research.
                            Xin has served as Area Chair for conferences such as ACL, NAACL, EMNLP, ICLR, and NeurIPS, as well as a Senior Program Committee for AAAI and IJCAI. He organized workshops and tutorials at conferences such as ACL, NAACL, CVPR, and ICCV. He has received several awards and recognitions for his work, including CVPR Best Student Paper Award, Google Research Faculty Award, Amazon Alexa Prize Awards, and various gift awards from Adobe, Snap, eBay, etc.</p>
                    </div>
                </div> -->

            </div>
        </div>
    </div>
</div>

<!-- Organizers -->
<div id="organizers" class="content-section-b">

    <div class="container">
        <div class="row wow "  data-animation-delay="200">
            <h3 class="section-heading">Organizers</h3>

            <div class="col-sm-2 wow "  data-animation-delay="200">
                <center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/organizer/jing.png" alt=""></center>
                <h4 class="section-heading"><center><a href="https://g-jing.github.io/">Jing Gu</a></center></h4>
                <h5 class="section-heading"><center><i>UC Santa Cruz</i></center></h5>
            </div>

            <div class="col-sm-2 wow "  data-animation-delay="200">
                <center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/organizer/ray.jpg" alt=""></center>
                <h4 class="section-heading"><center><a href="https://tsujuifu.github.io/">Tsu-Jui (Ray) Fu </a></center></h4>
                <h5 class="section-heading"><center><i>UC Santa Barbara</i></center></h5>
            </div>

            <div class="col-sm-2 wow "  data-animation-delay="200">
                <center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/organizer/drew.jpg" alt=""></center>
                <h4 class="section-heading"><center><a href="https://cs.stanford.edu/people/dorarad/">Drew Hudson</a></center></h4>
                <h5 class="section-heading"><center><i>Google DeepMind</i></center></h5>
            </div>

            <div class="col-sm-2 wow "  data-animation-delay="200">
                <center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/organizer/asli.jpg" alt=""></center>
                <h4 class="section-heading"><center><a href="http://asli.us/">Asli Celikyilmaz </a></center></h4>
                <h5 class="section-heading"><center><i>Fundamentals AI Research (FAIR) @ Meta</i></center></h5>
            </div>
            <!-- </div> -->

            <!-- <div class="row wow "  data-animation-delay="200"> -->
            <div class="col-sm-2 wow "  data-animation-delay="200">
                <center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/organizer/william.jpg" alt=""></center>
                <h4 class="section-heading"><center><a href="https://sites.cs.ucsb.edu/~william/">William Wang</a></center></h4>
                <h5 class="section-heading"><center><i>UC Santa Barbara</i></center></h5>
            </div>

            <div class="col-sm-2 wow "  data-animation-delay="200">
                <center><img  class="img-responsive img-rounded" style="border-radius: 50%" height="150" width="150" src="img/organizer/xin.jpg" alt=""></center>
                <h4 class="section-heading"><center><a href="https://eric-xw.github.io/">Xin Eric Wang</a></center></h4>
                <h5 class="section-heading"><center><i>UC Santa Cruz</i></center></h5>
            </div>        
        </div>
        <div class="wow " data-animation-delay="200">
          <h3 class="section-heading" style="color:white">Contact</h3>
          <!--div class="sub-title lead3">Lorem ipsum dolor sit atmet sit dolor greand fdanrh<br> sdfs sit atmet sit dolor greand fdanrh sdfs</div-->
          <p class="lead"  style="text-align:left;color:white">
          <!--Contact the Organizing Committee: alvr2024_naacl2024@softconf.com-->
          </p>

    </div>
    </div>
</div>

<!-- Program Committee -->

<div id ="committee" class="content-section-b" style="border-top: 0">
    <div class="container">
        <div class="row">

            <div class="col-sm-6 pull-right wow ">
                <img class="img-responsive " src="img/ipad.png" alt="">
            </div>

            <div class="wow " data-animation-delay="200">
                <h3 class="section-heading"> Program Committee</h3>

                <!--div class="sub-title lead3">Lorem ipsum dolor sit atmet sit dolor greand fdanrh<br> sdfs sit atmet sit dolor greand fdanrh sdfs</div-->
                <table cellspacing="0" cellpadding="0" style="margin-left: auto;margin-right: auto;width:75%">
                    <tr><td><li>Asma Ben Abacha</td><td>Microsoft</td></li><td></td></tr>
                    <tr><td><li>Shubham Agarwal</td><td>Mila</td></li><td></td></tr>
                    <tr><td><li>Arjun Akula</td><td>Google</td></li><td></td></tr>
                    <tr><td><li>Dhivya Chinnappa</td><td>Thomson Reuters</td></li><td></td></tr>
                    <tr><td><li>Simon Dobnik</td><td>University of Gothenburg</td></li><td></td></tr>
                    <tr><td><li>Yue Fan</td><td>University of California, Santa Cruz</td></li><td></td></tr>
                    <tr><td><li>Zhe Gan</td><td>Apple AI/ML</td></li><td></td></tr>
                    <tr><td><li>Cristina Garbacea</td><td>University of Michigan</td></li><td></td></tr>
                    <tr><td><li>Huaizu Jiang</td><td>Northeastern University</td></li><td></td></tr>
                    <tr><td><li>Yujie Lu</td><td>University of California, Santa Barbara</td></li><td></td></tr>
                    <tr><td><li>Loitongbam Sanayai Meetei</td><td>National Institute of Technology Silchar, India</td></li><td></td></tr>
                    <tr><td><li>Yulei Niu</td><td>Columbia University</td></li><td></td></tr>
                    <tr><td><li>Vikas Raunak</td><td>Microsoft</td></li><td></td></tr>
                    <tr><td><li>Arka Sadhu</td><td>University of Southern California</td></li><td></td></tr>
                    <tr><td><li>Thoudam Doren Singh</td><td>National Institute of Technology, Silchar, India</td></li><td></td></tr>
                    <tr><td><li>Alok Singh</td><td>National Institute of Technology, Silchar India</td></li><td></td></tr>
                    <tr><td><li>Ece Takmaz</td><td>University of Amsterdam</td></li><td></td></tr>
                    <tr><td><li>Hao Tan</td><td>Adobe Research</td></li><td></td></tr>
                    <tr><td><li>Yiming Xie</td><td>Northeastern University</td></li><td></td></tr>
                    <tr><td><li>Qianqi Yan</td><td>University of California, Santa Cruz</td></li><td></td></tr>
                    <tr><td><li>Kaizhi Zheng</td><td>University of California, Santa Cruz</td></li><td></td></tr>
                    <tr><td><li>Wanrong Zhu</td><td>University of California, Santa Barbara</td></li><td></td></tr>                    
                </table>
                <br/>

                <!--p><a class="btn btn-embossed btn-primary" href="#" role="button">View Details</a>
                <a class="btn btn-embossed btn-info" href="#" role="button">Visit Website</a></p-->
            </div>
        </div>
    </div>

    <!-- /.container -->


    <!-- Sponsor -->
    <!-- <div id ="sponsors" class="content-section-a" style="border-top: 0"> -->
    <!-- <div class="container">			 -->
    <!-- <div class="row">			 -->
    <!--div class="col-sm-6 pull-right wow ">
        <img class="img-responsive " src="img/ipad.png" alt="">
    </div-->
    <!-- <div class="wow " data-animation-delay="200">    -->
    <!-- <h3 class="section-heading"> Sponsor </h3> -->
    <!--div class="sub-title lead3">Lorem ipsum dolor sit atmet sit dolor greand fdanrh<br> sdfs sit atmet sit dolor greand fdanrh sdfs</div-->
    <!-- <p class="lead"  style="text-align:justify"> -->
    <!-- <a href=" " target="_blank"><img src="img/sponsor/XXX.png" width="300"></a> -->
    <!-- </p> -->
    <!--p><a class="btn btn-embossed btn-primary" href="#" role="button">View Details</a>
    <a class="btn btn-embossed btn-info" href="#" role="button">Visit Website</a></p-->
    <!-- </div>    -->
    <!-- </div> -->
    <!-- </div> -->
    <!-- /.container -->
    <!-- </div> -->

    <!--
    <div id="contacts" class="content-section-c ">
        <div class="container">
            <div class="row">

                <div class="wow " data-animation-delay="200">
                    <h3 class="section-heading" style="color:white">Contact</h3>
                    <p class="lead"  style="text-align:left;color:white">
                        Contact the Organizing Committee: alvr2024_naacl2024@softconf.com
                    </p>

                </div>
            </div>
            <div class="row">

                <div class="col-md-6 col-md-offset-3 text-center">
                    <div >
                        <div class="morph-button ">
                            <button type="button"></button>

                        </div>
                    </div>
                </div>
            </div>

        </div>
    </div>
    -->

    <footer>

    </footer>

    <!-- JavaScript -->
    <script src="js/jquery-1.10.2.js"></script>
    <script src="js/bootstrap.min.js"></script>
    <script src="js/owl.carousel.js"></script>
    <script src="js/script.js"></script>
    <!-- StikyMenu -->
    <script src="js/stickUp.min.js"></script>
    <script type="text/javascript">
      jQuery(function($) {
        $(document).ready( function() {
          $('.navbar-default').stickUp();

        });
      });

    </script>
    <!-- Smoothscroll -->
    <script type="text/javascript" src="js/jquery.corner.js"></script>
    <script src="js/wow.min.js"></script>
    <script>
      new WOW().init();
    </script>
    <script src="js/classie.js"></script>
    <script src="js/uiMorphingButton_inflow.js"></script>
    <!-- Magnific Popup core JS file -->
    <script src="js/jquery.magnific-popup.js"></script>
</body>

</html>
